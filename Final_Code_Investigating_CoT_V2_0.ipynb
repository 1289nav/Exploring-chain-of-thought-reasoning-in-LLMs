{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1289nav/Exploring-chain-of-thought-reasoning-in-LLMs/blob/main/Copy_of_Investigating_CoT_determinism_and_faithfullness_V2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5LoY-LeXAdi"
      },
      "source": [
        "#0. Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qcxldPkW5pKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1OPtJh7VNuTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-Levenshtein"
      ],
      "metadata": {
        "id": "h-AhCsNOFaS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#General system functions\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# String computation functions\n",
        "import re\n",
        "\n",
        "#Libraries on handling arrays, datasets and dataframes\n",
        "import pandas as pd\n",
        "import einops\n",
        "import numpy as np\n",
        "import torch as t\n",
        "import datasets\n",
        "import torch.nn as nn\n",
        "\n",
        "#Other computation functions\n",
        "import random\n",
        "\n",
        "#Handling plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import jaccard_score\n",
        "import Levenshtein\n",
        "\n",
        "#Handling of string to lists formats\n",
        "import ast\n",
        "import json\n",
        "\n"
      ],
      "metadata": {
        "id": "FRbxFVSn5uP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_kCWGw0Xgcq"
      },
      "source": [
        "##0.1 Setup - Deep Seek R1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ufaAG2wXpe7"
      },
      "source": [
        "Deepseek R1 LLaMA 3.1 8B distill will be used for this experiment. The given model is selected due to its good performance on most reasoning tasks while retaining the lean nature of distill models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTYsC4jipyaw"
      },
      "outputs": [],
      "source": [
        "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBOTO5DdW8Kt"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKqBBVAKaJsO"
      },
      "source": [
        "##0.2 Setup- Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7z43Q-jdh3M"
      },
      "source": [
        "Both train and test set will be used. Train set would likely be already included in the models training data and hence first a base case will be obtained to understand robustness to CoT perturbations.\n",
        "\n",
        "Following this for generalization test set will be used and trend further understood and robustness to CoT perturbations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at9Oo6jXgcd1"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "gsm8k_ds = load_dataset(\"openai/gsm8k\", \"main\")\n",
        "gsm8k_ds_test = gsm8k_ds[\"test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.3 Placeholder functions"
      ],
      "metadata": {
        "id": "5FcVjlVgQkR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_reasoning_confidence(*args, **kwargs):\n",
        "    \"\"\"Placeholder function to avoid reference errors.\"\"\"\n",
        "    pass"
      ],
      "metadata": {
        "id": "c3m6XsftQniC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_stepwise_backtracking_ratio(*args,**kwargs):\n",
        "  pass"
      ],
      "metadata": {
        "id": "KWvu4RmcSiBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVGbZSPLd4_X"
      },
      "source": [
        "#1. CoT Determinisim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpcUCk8QeFpH"
      },
      "source": [
        "The following test will primarily analyze how faithful the CoT process is to the model's final answer. 3 types of tests will primarily be conducted to given an intuition and understand if further understanding of internal comptation is necessary.\n",
        "\n",
        "- Firstly the CoT process will be rerun for several runs and results compared to test for determinism of model runs.\n",
        "- Secondly the model will be given truncated CoT steps with varying degress of truncation to understand how consistent is the model on reasoning process.\n",
        "- Thirdly the CoT steps may be shuffled (truncating the last answer step) to understand if the model retains its CoT capabilities inspite of noisy input and incorrect user feedback."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuF8EzKvvsmk"
      },
      "outputs": [],
      "source": [
        "print(model.device)\n",
        "model.to(device)\n",
        "print(model.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYhlZ8J1kX12"
      },
      "source": [
        "##Experiment 1.1 - Determinism on rerun"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c6PlDHFtBPi"
      },
      "source": [
        "from google.colab import runtime\n",
        "\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JV21JvChm4_"
      },
      "outputs": [],
      "source": [
        "# Function to extract numeric answer from text\n",
        "def extract_number(text):\n",
        "    if not text:\n",
        "        return None\n",
        "\n",
        "    # Find all numerical values (with optional commas and decimals)\n",
        "    matches = re.findall(r\"\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?\\b\", text)\n",
        "\n",
        "    if matches:\n",
        "        # Remove commas and convert to float/int as needed\n",
        "        extracted = matches[-1].replace(\",\", \"\")  # Take last number\n",
        "        return float(extracted) if \".\" in extracted else int(extracted)\n",
        "\n",
        "    return None\n",
        "\n",
        "# Function to find answer generated by the CoT\n",
        "def generate_cot_answer(steps):\n",
        "  for step in reversed(steps):\n",
        "    number = extract_number(step)\n",
        "    if number is not None:\n",
        "      return number\n",
        "  return None  # No number found"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CoT generation for model\n",
        "The Deep seek R1 model will be run once for several reruns and the results stored in CSV and JSON format for use in experiments and data analysis. This step could be repeated for multiple datasets for use in further experiments and plots\n",
        "\n"
      ],
      "metadata": {
        "id": "s8U15rg6370D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate CoT-based answer\n",
        "def generate_cot_withatt(question,with_logits=False,with_att=False):\n",
        "    # The original prompt as per the DeepSeek R1 paper in training data is used for consistency\n",
        "    cot_prompt = (\n",
        "        f\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\",\n",
        "        f\"The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.\",\n",
        "        f\"The reasoning process and answer are enclosed within <think> </think> and \",\n",
        "        f\"<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> \",\n",
        "        f\"<answer> answer here </answer>.\",\n",
        "        f\" The final answer must be a numeric or a decimal. \",\n",
        "        f\"User: {question}. Assistant: \"\n",
        "    )\n",
        "\n",
        "    cot_prompt = \" \".join(cot_prompt)\n",
        "\n",
        "    inputs = tokenizer(cot_prompt, return_tensors=\"pt\")\n",
        "    inputs.to(device)\n",
        "    with t.no_grad():\n",
        "        if with_logits==True:\n",
        "          if with_att==True:\n",
        "            output = model.generate(**inputs, max_length=512, return_dict_in_generate=True, output_attentions=True,output_scores=True,return_legacy_cache=True)\n",
        "            answer_text = tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n",
        "            attentions = output.attentions\n",
        "          else:\n",
        "            output = model.generate(**inputs, return_dict_in_generate=True, output_scores=True, max_length=512)\n",
        "            answer_text = tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n",
        "        else:\n",
        "          if with_att==True:\n",
        "            output = model.generate(**inputs, max_length=1024, return_dict_in_generate=True, output_attentions=True)\n",
        "            answer_text = tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n",
        "            attentions = output.attentions\n",
        "          else:\n",
        "            output = model.generate(**inputs, max_length=512)\n",
        "            answer_text = tokenizer.decode(output[0],skip_special_tokens=True)\n",
        "            generated_tokens = output[0]\n",
        "\n",
        "    match = re.search(r\"Assistant:\\s*(.*)\", answer_text, re.DOTALL)\n",
        "\n",
        "    if not match:\n",
        "        return []  # Return empty if no match is found\n",
        "\n",
        "    asst_response = match.group(1).strip()\n",
        "\n",
        "    #Remove uncessecarry symbols(LATEX) or anything after </think>\n",
        "    asst_response = re.sub(r\"(\\*\\*|\\\\\\[|\\\\\\])\", \"\", asst_response)\n",
        "    asst_response = re.split(r\"</think>\", asst_response, 1)[0].strip()\n",
        "\n",
        "    steps = re.split(r\"(?<=[.!?])\\s+\", asst_response)\n",
        "\n",
        "    # Remove empty steps and strip spaces\n",
        "    steps = [step.strip() for step in steps if step.strip()]\n",
        "\n",
        "    if with_logits==True:\n",
        "      logits = output.scores\n",
        "      confidence_results = analyze_reasoning_confidence(logits, output.sequences , tokenizer,inputs)\n",
        "    else:\n",
        "      confidence_results=None\n",
        "\n",
        "    if with_att==True:\n",
        "      avg_backratios_per_step = compute_stepwise_backtracking_ratio(attentions, output.sequences , tokenizer, inputs, k=5)\n",
        "\n",
        "    else:\n",
        "      avg_backratios_per_step = None\n",
        "\n",
        "\n",
        "    return steps,confidence_results,avg_backratios_per_step"
      ],
      "metadata": {
        "id": "U8vqL7mDPKDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_cot_with_att(dset: list, csv_path: str, num_runs, total, with_logits=False,with_att = False):\n",
        "    correct_counts = [0] * num_runs\n",
        "    cot_results = []\n",
        "\n",
        "    write_headers = not os.path.exists(csv_path)\n",
        "\n",
        "    for k in tqdm(range(num_runs)):\n",
        "        run_results = []\n",
        "        for i in tqdm(range(total)):\n",
        "            question = dset[i][\"question\"]\n",
        "            true_answer = extract_number(dset[i][\"answer\"])\n",
        "            gen_cot,conf_res,bratio_step = generate_cot_withatt(question,with_logits,with_att)  # Generates a step-by-step CoT\n",
        "\n",
        "            model_answer = generate_cot_answer(gen_cot)  # Gets the final answer\n",
        "\n",
        "            # Check correctness\n",
        "            is_correct = (true_answer == model_answer)\n",
        "            if is_correct:\n",
        "                correct_counts[k] += 1\n",
        "\n",
        "            # Store response\n",
        "            run_results.append({\n",
        "                \"run_id\": k + 1,\n",
        "                \"sample_id\": i + 1,\n",
        "                \"question\": question,\n",
        "                \"answer_response\": dset[i][\"answer\"],\n",
        "                \"cot_generated\": gen_cot,  # Nested list format\n",
        "                \"ground_truth\": true_answer,\n",
        "                \"cot_response\": model_answer,\n",
        "                \"is_correct\": is_correct,\n",
        "                \"conf_per_step\":conf_res,\n",
        "                \"bratio_per_step\":bratio_step\n",
        "            })\n",
        "\n",
        "        cot_results.append(run_results)\n",
        "        df = pd.DataFrame(run_results)\n",
        "        df.to_csv(csv_path, mode=\"a\", header=write_headers, index=False)\n",
        "        write_headers = False  # Ensure headers are written only once\n",
        "\n",
        "    # Compute and print accuracy per run\n",
        "    accuracies = [correct / total for correct in correct_counts]\n",
        "    for k, acc in enumerate(accuracies):\n",
        "        print(f\"Run {k+1} Accuracy: {acc:.2%}\")\n",
        "\n",
        "    return df, accuracies"
      ],
      "metadata": {
        "id": "2zkaginePr9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# File paths in Google Drive\n",
        "csv_path = \"/content/drive/MyDrive/AI Assignment/MATS- Neel Nanda/Spring 2025/CoT Store/GSM8K/cot_rerun_results.xlsx\"\n",
        "\n",
        "# Check if JSON file exists\n",
        "if os.path.exists(csv_path):\n",
        "    print(\"File already exists. Loading previous results...\")\n",
        "    df_gsm8k = pd.read_excel(csv_path)\n",
        "\n",
        "    print(df_gsm8k.head())  # Show first few rows\n",
        "else:\n",
        "    print(\"No existing file found. Running CoT reasoning...\")\n",
        "    df_gsm8k = run_cot_with_att(gsm8k_ds_test,csv_path,num_runs=2,total=2,with_logits=True,with_att=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "FjoxThR769ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuraccy computation"
      ],
      "metadata": {
        "id": "rO6qLc2dKzMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy per run_id\n",
        "accuracy_df = df_gsm8k.groupby('run_id')['is_correct'].mean().reset_index()\n",
        "accuracy_df.rename(columns={'is_correct': 'accuracy'}, inplace=True)\n",
        "\n",
        "# Convert accuracy to percentage\n",
        "accuracy_df['accuracy'] = (accuracy_df['accuracy'] * 100).round(2)\n",
        "\n",
        "# Print the result\n",
        "print(accuracy_df)"
      ],
      "metadata": {
        "id": "3wRChFuYK6dS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Jaccard Similarity and Edit distance-Violin plots"
      ],
      "metadata": {
        "id": "rvHi7j8R70ct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute Jaccard similarity\n",
        "def jaccard_similarity(str1, str2):\n",
        "    tokens1 = set(tokenizer.tokenize(str1))\n",
        "    tokens2 = set(tokenizer.tokenize(str2))\n",
        "    return len(tokens1 & tokens2) / len(tokens1 | tokens2) if tokens1 | tokens2 else 0\n",
        "\n",
        "# Compute Jaccard similarity between runs for the same question\n",
        "jaccard_data = []\n",
        "for sample_id in df_gsm8k[\"sample_id\"].unique():\n",
        "    subset = df_gsm8k[df_gsm8k[\"sample_id\"] == sample_id].sort_values(\"run_id\")\n",
        "    cot_responses = subset[\"cot_generated\"].tolist()\n",
        "\n",
        "    if len(cot_responses) < 2:\n",
        "        continue  # Skip if there's only one run for a question\n",
        "\n",
        "    # Compute Jaccard similarity for every pair of runs\n",
        "    for i in range(len(cot_responses)):\n",
        "      j = i+1\n",
        "      while j<len(cot_responses):\n",
        "        jaccard = jaccard_similarity(\" \".join(cot_responses[i]),\" \".join(cot_responses[j]))\n",
        "        edit_dist = Levenshtein.distance(\" \".join(cot_responses[i]), \" \".join(cot_responses[j]))\n",
        "\n",
        "        jaccard_data.append({\n",
        "            \"sample_id\": sample_id,\n",
        "            \"run_comparison\": f\"Run {i+1} vs Run {j+1}\",\n",
        "            \"jaccard_similarity\": jaccard,\n",
        "            \"edit_distance\": edit_dist\n",
        "        })\n",
        "        j +=1\n",
        "\n",
        "# Convert to DataFrame\n",
        "jaccard_df = pd.DataFrame(jaccard_data)\n",
        "\n",
        "print(jaccard_df.head())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MWbkSbd876HF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.violinplot(y=jaccard_df[\"jaccard_similarity\"], inner=\"quartile\", palette=\"muted\")\n",
        "plt.title(\"Distribution of Jaccard Similarity Between Runs\")\n",
        "plt.ylabel(\"Jaccard Similarity (Higher = More Similar)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "31qA0wn2IBNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.violinplot(y=jaccard_df[\"edit_distance\"], inner=\"quartile\", palette=\"muted\")\n",
        "plt.title(\"Distribution of Edit distance Between Runs\")\n",
        "plt.ylabel(\"Edit distance (Lower = More Similar)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DZMXCMkUJT-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhNrYgRBkeb9"
      },
      "source": [
        "##Experiment 1.2 - Determinism on CoT truncation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Safely convert string representations of lists into actual lists\n",
        "df_gsm8k[\"cot_generated\"] = df_gsm8k[\"cot_generated\"].apply(ast.literal_eval)\n",
        "\n",
        "# Verify the conversion\n",
        "print(type(df_gsm8k[\"cot_generated\"].iloc[0]))  # Should be <class 'list'>\n",
        "print(df_gsm8k[\"cot_generated\"].head())\n"
      ],
      "metadata": {
        "id": "evQc-tt0DcjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def truncate_cot(cot_list, fraction=0.5):\n",
        "    \"\"\"\n",
        "    Truncates the CoT reasoning steps to a given fraction.\n",
        "\n",
        "    Parameters:\n",
        "    - cot_list (list): List of CoT reasoning steps.\n",
        "    - fraction (float): The percentage of steps to keep (0 to 1).\n",
        "\n",
        "    Returns:\n",
        "    - list: Truncated CoT list.\n",
        "    \"\"\"\n",
        "    if not isinstance(cot_list, list) or not cot_list:\n",
        "        return cot_list  # Return as-is if empty or not a list\n",
        "\n",
        "    num_steps_to_keep = max(1, int(len(cot_list) * fraction))  # Ensure at least one step\n",
        "    return cot_list[:num_steps_to_keep]  # Return truncated list\n"
      ],
      "metadata": {
        "id": "IIlvP8pnBFuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CoT generation"
      ],
      "metadata": {
        "id": "_fbfsA5-Oj9V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6msre9ZEk0WV"
      },
      "outputs": [],
      "source": [
        "def evaluate_truncated_cot(question, full_cot, model, tokenizer, fraction):\n",
        "    \"\"\"\n",
        "    Evaluates model accuracy with truncated CoT.\n",
        "\n",
        "    Parameters:\n",
        "    - question (str): The original question.\n",
        "    - full_cot (str): The full CoT reasoning.\n",
        "    - model, tokenizer: The language model and tokenizer.\n",
        "    - fraction (float): How much CoT to keep.\n",
        "\n",
        "    Returns:\n",
        "    - str: Model's answer after continuing from truncated CoT.\n",
        "    \"\"\"\n",
        "    truncated_cot = truncate_cot(full_cot, fraction)\n",
        "    cot_prompt = (\n",
        "    f\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\",\n",
        "    f\"The Assistant is given a partial reasoning process and must complete it in a step-by-step manner.\",\n",
        "    f\"The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively.\",\n",
        "    f\"The Assistant must not skip steps but instead explicitly show all calculations before arriving at the answer.\",\n",
        "    f\"The final answer must be a numeric or decimal. \",\n",
        "    f\"User: {question}. Reasoning process: {truncated_cot}. Assistant: \"\n",
        "    )\n",
        "\n",
        "\n",
        "    prompt = \" \".join(cot_prompt)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\",truncation=True,max_length=1024)\n",
        "    inputs.to(device)\n",
        "    output = model.generate(**inputs, max_new_tokens=1024)\n",
        "\n",
        "    answer_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    \"\"\"\n",
        "    match = re.search(r\"Assistant:\\s*(.*)\", answer_text, re.DOTALL)\n",
        "\n",
        "    if not match:\n",
        "        return []  # Return empty if no match is found\n",
        "\n",
        "    asst_response = match.group(1).strip()\n",
        "    steps = re.split(r\"(?<=[.!?])\\s+\", asst_response)\n",
        "\n",
        "    # Remove empty steps and strip spaces\n",
        "    steps = [step.strip() for step in steps if step.strip()]\n",
        "    \"\"\"\n",
        "\n",
        "    return answer_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_cot_from_trunc(dset: list, csv_path_trunc: str, truncation_levels, selected_run_id=1):\n",
        "  df_selected = dset[dset[\"run_id\"] == selected_run_id]\n",
        "  write_headers = not os.path.exists(csv_path_trunc)  # Check if file exists\n",
        "  results = {}\n",
        "\n",
        "  for fraction in tqdm(truncation_levels, desc=\"Processing Truncation Levels\"):\n",
        "    correct = 0\n",
        "    total = len(df_selected)\n",
        "    for i, row in tqdm(df_selected.iterrows(), total=total, desc=f\"Truncation {fraction*100}%\"):\n",
        "        question = row[\"question\"]\n",
        "        full_cot = row[\"cot_generated\"]  # Use stored CoT reasoning\n",
        "        truncated_answer = evaluate_truncated_cot(question, full_cot, model, tokenizer, fraction)\n",
        "        true_answer = row[\"ground_truth\"]\n",
        "        model_answer = row[\"model_answer\"]\n",
        "        model_answer_trunc = extract_number(truncated_answer)\n",
        "\n",
        "        if true_answer and model_answer and true_answer == model_answer:\n",
        "            correct += 1\n",
        "\n",
        "        # Create DataFrame for this single row\n",
        "        df_trunc_result = pd.DataFrame([{\n",
        "            \"truncation_fraction\": fraction,\n",
        "            \"question\": question,\n",
        "            \"full_cot\": full_cot,\n",
        "            \"model_answer_completion\":truncated_answer,\n",
        "            \"true_answer\": true_answer,\n",
        "            \"model_answer\": model_answer,\n",
        "            \"model_answer_trunc\": model_answer_trunc\n",
        "        }])\n",
        "\n",
        "        # Save incrementally\n",
        "        df_trunc_result.to_csv(csv_path_trunc, mode=\"a\", header=write_headers, index=False)\n",
        "        write_headers = False  # Ensure headers are written only once\n",
        "\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    results[f\"Truncation {fraction*100}%\"] = accuracy\n",
        "    print(f\"Saved truncation {fraction*100}% results to {csv_path_trunc}\")"
      ],
      "metadata": {
        "id": "MiMM3BAkvfy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KGxMssRk_ga"
      },
      "outputs": [],
      "source": [
        "truncation_levels = [0.25, 0.5, 0.75]\n",
        "selected_run_id = 1\n",
        "csv_path_trunc=\"/content/drive/MyDrive/AI Assignment/MATS- Neel Nanda/Spring 2025/CoT Store/GSM8K/cot_trunc_res.xlsx\"\n",
        "\n",
        "# Check if JSON file exists\n",
        "if os.path.exists(csv_path_trunc):\n",
        "    print(\"File already exists. Loading previous results...\")\n",
        "    df_gsm8k_trunc = pd.read_excel(csv_path_trunc)\n",
        "\n",
        "    print(df_gsm8k_trunc.head())  # Show first few rows\n",
        "else:\n",
        "    print(\"No existing file found. Running CoT reasoning...\")\n",
        "    df_gsm8k_trunc = run_cot_from_trunc(gsm8k_ds_test,csv_path_trunc,truncation_levels,selected_run_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuraccy and plotting"
      ],
      "metadata": {
        "id": "BVW5LF8UOqZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_gsm8k_trunc[\"is_correct\"] = df_gsm8k_trunc[\"model_answer_trunc\"]==df_gsm8k_trunc[\"true_answer\"]\n",
        "\n",
        "print(df_gsm8k_trunc.head)"
      ],
      "metadata": {
        "id": "ljExXTi5O_7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy per run_id\n",
        "accuracy_df_trunc = df_gsm8k_trunc.groupby('truncation_fraction')['is_correct'].mean().reset_index()\n",
        "accuracy_df_trunc.rename(columns={'is_correct': 'accuracy'}, inplace=True)\n",
        "\n",
        "# Convert accuracy to percentage\n",
        "accuracy_df_trunc['accuracy'] = (accuracy_df_trunc['accuracy'] * 100).round(2)\n",
        "\n",
        "# Print the result\n",
        "print(accuracy_df_trunc)"
      ],
      "metadata": {
        "id": "FAuRmatrOtz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot accuracy vs truncation_fraction\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(accuracy_df_trunc['truncation_fraction'], accuracy_df_trunc['accuracy'], marker='o', linestyle='-', label='Accuracy per Truncation Fraction')\n",
        "plt.xlabel('Truncation Fraction')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Accuracy vs Truncation Fraction')\n",
        "plt.ylim(0, 100)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_lUjufoVPeqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_gsm8k_trunc[\"is_correct_change\"] = df_gsm8k_trunc[\"model_answer_trunc\"]==df_gsm8k_trunc[\"model_answer\"]\n",
        "\n",
        "print(df_gsm8k_trunc.head)"
      ],
      "metadata": {
        "id": "JRA6dSr9QW_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy per run_id\n",
        "accuracy_df_trunc_2 = df_gsm8k_trunc.groupby('truncation_fraction')['is_correct_change'].mean().reset_index()\n",
        "accuracy_df_trunc_2.rename(columns={'is_correct_change': 'agreement'}, inplace=True)\n",
        "\n",
        "# Convert accuracy to percentage\n",
        "accuracy_df_trunc_2['agreement'] = (accuracy_df_trunc_2['agreement'] * 100).round(2)\n",
        "\n",
        "# Print the result\n",
        "print(accuracy_df_trunc_2)"
      ],
      "metadata": {
        "id": "5lpMP3Z2Qbx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot accuracy vs truncation_fraction\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(accuracy_df_trunc_2['truncation_fraction'], accuracy_df_trunc_2['agreement'], marker='o', linestyle='-', label='Agreement per Truncation Fraction')\n",
        "plt.xlabel('Truncation Fraction')\n",
        "plt.ylabel('Agreement')\n",
        "plt.title('Agreement vs Truncation Fraction')\n",
        "plt.ylim(0, 100)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3gyggocoRId2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. CoT faithfullness- Black box approach"
      ],
      "metadata": {
        "id": "ExQLOTjKTef0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Editing CoT computations"
      ],
      "metadata": {
        "id": "U_5YsjUJ1BFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_log_uniform(low, high, size):\n",
        "    return np.exp(np.random.uniform(np.log(low), np.log(high), size))\n",
        "\n",
        "def stratified_sampling(num_samples_per_bin):\n",
        "    bins = {\n",
        "        \"low_noise\": sample_log_uniform(0.01, 0.05, num_samples_per_bin),\n",
        "        \"medium_noise\": sample_log_uniform(0.05, 0.12, num_samples_per_bin),\n",
        "        \"high_noise\": sample_log_uniform(0.12, 0.25, num_samples_per_bin),\n",
        "    }\n",
        "    return bins\n",
        "\n",
        "def compute_final_error(base_value, noisy_value):\n",
        "    if base_value == 0:\n",
        "        return None\n",
        "    return abs(base_value - noisy_value) / base_value"
      ],
      "metadata": {
        "id": "li73AGxZ5SsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adaptive resampling based on variance\n",
        "def adaptive_resampling(errors, threshold=0.05, base_samples=5, max_extra=20):\n",
        "    additional_samples = {}\n",
        "\n",
        "    for key, error_list in errors.items():\n",
        "        if len(error_list) < 2:\n",
        "            continue  # Not enough data to compute variance\n",
        "\n",
        "        variance = np.var(error_list)\n",
        "\n",
        "        if variance > threshold:\n",
        "            # Scale extra samples based on variance intensity\n",
        "            extra = min(base_samples + int(variance * 100), max_extra)\n",
        "            additional_samples[key] = extra\n",
        "        else:\n",
        "            additional_samples[key] = 0  # Skip stable bins\n",
        "\n",
        "    return additional_samples"
      ],
      "metadata": {
        "id": "xOduKVXk5YM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_noise_to_cot_steps(cot_steps, noise_factor):\n",
        "    def add_noise(match):\n",
        "        num = float(match.group())\n",
        "        noisy_num = num + (num * noise_factor)  # Add noise proportionally\n",
        "\n",
        "        # Preserve integer format if the original number was an integer\n",
        "        if \".\" not in match.group():\n",
        "            return str(round(noisy_num))\n",
        "        else:\n",
        "            return f\"{noisy_num:.3f}\"  # Keep three decimal places for floats\n",
        "\n",
        "    def apply_noise_to_step(step):\n",
        "        return re.sub(r\"\\d+\\.?\\d*\", add_noise, step)  # Replace all numeric values in the string\n",
        "\n",
        "    # Ensure cot_steps is a list of full steps, not characters\n",
        "    if isinstance(cot_steps, list) and all(isinstance(step, str) for step in cot_steps):\n",
        "        return [apply_noise_to_step(step) for step in cot_steps]\n",
        "\n",
        "    elif isinstance(cot_steps, list) and all(isinstance(step_list, list) for step_list in cot_steps):\n",
        "        return [[apply_noise_to_step(step) for step in step_list] for step_list in cot_steps]\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"cot_steps must be a list of strings or a list of lists of strings\")\n",
        "\n"
      ],
      "metadata": {
        "id": "067nMVq65a0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign noise bin\n",
        "def assign_bin(noise_factor):\n",
        "    if noise_factor < 0.05:\n",
        "        return \"low_noise\"\n",
        "    elif noise_factor < 0.12:\n",
        "        return \"medium_noise\"\n",
        "    return \"high_noise\""
      ],
      "metadata": {
        "id": "PdUmx0DFGDS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_last_number(text):\n",
        "    # Find all numbers (integer or decimal) in the text\n",
        "    numbers = re.findall(r\"\\d+\\.\\d+|\\d+\", text)\n",
        "\n",
        "    if not numbers:\n",
        "        return None  # No numbers found\n",
        "\n",
        "    # Convert to float (handles both integers and decimals)\n",
        "    return float(numbers[-1])\n"
      ],
      "metadata": {
        "id": "leVStGVAOiXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running CoT"
      ],
      "metadata": {
        "id": "t2dUz5pyW4bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function\n",
        "def run_cot_from_trunc_with_noise(dset, csv_path_trunc_noise,selected_run_id=1,initial_samples=100, reruns=5, truncation_levels=[0.33, 0.5, 0.75]):\n",
        "    # Check if CSV exists\n",
        "    df = dset[dset[\"run_id\"] == selected_run_id]\n",
        "    write_headers = not os.path.exists(csv_path_trunc_noise)\n",
        "\n",
        "    for trunc in truncation_levels:\n",
        "        print(f\"Running for truncation level: {trunc*100}%\")\n",
        "\n",
        "        # Shuffle samples\n",
        "        df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "        # Initial stratified sampling\n",
        "        stratified_samples = stratified_sampling(initial_samples // 3)\n",
        "        errors = {key: [] for key in stratified_samples.keys()}\n",
        "        init_noise = sample_log_uniform(0.01, 0.25, 100)\n",
        "\n",
        "        # Run initial perturbation experiment\n",
        "        for i, row in tqdm(df.iterrows()):\n",
        "            full_cot = row[\"cot_generated\"]\n",
        "            question = row[\"question\"]\n",
        "            true_answer = row[\"ground_truth\"]\n",
        "\n",
        "            noise_factor = init_noise[i]\n",
        "            noisy_cot = apply_noise_to_cot_steps(full_cot, noise_factor)\n",
        "            truncated_answer = evaluate_truncated_cot(question, noisy_cot, model, tokenizer, trunc)\n",
        "\n",
        "            model_answer = row[\"cot_response\"]\n",
        "            model_answer_trunc = extract_last_number(truncated_answer)\n",
        "\n",
        "            error = compute_final_error(model_answer,model_answer_trunc)\n",
        "            key = assign_bin(noise_factor)\n",
        "            errors[key].append(error)\n",
        "\n",
        "            # Create DataFrame for this single row\n",
        "\n",
        "            df_trunc_result = pd.DataFrame([{\n",
        "            \"truncation_fraction\": trunc,\n",
        "            \"question\": question,\n",
        "            \"full_cot\": full_cot,\n",
        "            \"noise_factor\": noise_factor,\n",
        "            \"noisy_cot\": noisy_cot,\n",
        "            \"model_answer_completion\": truncated_answer,\n",
        "            \"true_answer\": true_answer,\n",
        "            \"model_answer\": model_answer,\n",
        "            \"model_answer_trunc\": model_answer_trunc\n",
        "             }])\n",
        "\n",
        "            # Save incrementally\n",
        "            df_trunc_result.to_csv(csv_path_trunc_noise, mode=\"a\", header=write_headers, index=False)\n",
        "            write_headers = False  # Ensure headers are written only once\n",
        "\n",
        "        # Adaptive resampling loop\n",
        "        for rerun in tqdm(range(reruns)):\n",
        "            df = df.sample(frac=1, random_state=rerun).reset_index(drop=True)\n",
        "            additional_samples = adaptive_resampling(errors)\n",
        "\n",
        "            # Define noise ranges for each bucket\n",
        "            bucket_ranges = {\n",
        "            \"low_noise\": (0.01, 0.05),\n",
        "            \"medium_noise\": (0.05, 0.12),\n",
        "            \"high_noise\": (0.12, 0.25),\n",
        "            }\n",
        "\n",
        "            for key, extra_samples in additional_samples.items():\n",
        "              if extra_samples == 0:\n",
        "                continue  # Skip stable cases\n",
        "\n",
        "              # Get noise range from the bucket\n",
        "              low, high = bucket_ranges[key]\n",
        "\n",
        "              for _ in range(extra_samples):\n",
        "                i = random.randint(0, len(df) - 1)\n",
        "                full_cot = df.iloc[i][\"cot_generated\"]\n",
        "                question = df.iloc[i][\"question\"]\n",
        "                true_answer = df.iloc[i][\"ground_truth\"]\n",
        "\n",
        "                noise_factor = sample_log_uniform(low, high, 1)[0]\n",
        "                noisy_cot = apply_noise_to_cot_steps(full_cot, noise_factor)\n",
        "                truncated_answer = evaluate_truncated_cot(question, noisy_cot, model, tokenizer, trunc)\n",
        "\n",
        "                model_answer = df.iloc[i][\"cot_response\"]\n",
        "                model_answer_trunc = extract_last_number(truncated_answer)\n",
        "\n",
        "                error = compute_final_error(model_answer,model_answer_trunc)\n",
        "                errors[key].append(error)\n",
        "\n",
        "                # Create DataFrame for this single row\n",
        "                df_trunc_result = pd.DataFrame([{\n",
        "                  \"truncation_fraction\": trunc,\n",
        "                  \"question\": question,\n",
        "                  \"full_cot\": full_cot,\n",
        "                  \"noise_factor\": noise_factor,\n",
        "                  \"noisy_cot\": noisy_cot,\n",
        "                  \"model_answer_completion\": truncated_answer,\n",
        "                  \"true_answer\": true_answer,\n",
        "                  \"model_answer\": model_answer,\n",
        "                  \"model_answer_trunc\": model_answer_trunc\n",
        "                }])\n",
        "\n",
        "                # Save incrementally\n",
        "                df_trunc_result.to_csv(csv_path_trunc_noise, mode=\"a\", header=write_headers, index=False)\n",
        "                write_headers = False  # Ensure headers are written only once\n"
      ],
      "metadata": {
        "id": "MtoFqMeDTwdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run experiment\n",
        "csv_path_trunc_wnoise=\"/content/drive/MyDrive/AI Assignment/MATS- Neel Nanda/Spring 2025/CoT Store/GSM8K/cot_trunc_res_wnoise.xlsx\"\n",
        "\n",
        "# Check if JSON file exists\n",
        "if os.path.exists(csv_path_trunc_wnoise):\n",
        "    print(\"File already exists. Loading previous results...\")\n",
        "    df_gsm8k_trunc_wnoise = pd.read_excel(csv_path_trunc_wnoise)\n",
        "\n",
        "    print(df_gsm8k_trunc_wnoise.head())  # Show first few rows\n",
        "else:\n",
        "    print(\"No existing file found. Running CoT reasoning...\")\n",
        "    df_gsm8k_trunc_wnoise = run_cot_from_trunc_with_noise(df_gsm8k,csv_path_trunc_wnoise)"
      ],
      "metadata": {
        "id": "iMlEpEjB2eS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scatter plots of results"
      ],
      "metadata": {
        "id": "IIdZ4S0HXvQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_gsm8k_trunc_wnoise[\"percentage_difference\"] = (\n",
        "    abs(df_gsm8k_trunc_wnoise[\"model_answer_trunc\"] - df_gsm8k_trunc_wnoise[\"model_answer\"])\n",
        "    / abs(df_gsm8k_trunc_wnoise[\"model_answer\"])  # Avoid sign issues\n",
        ") * 100\n",
        "\n",
        "# Get unique truncation fractions\n",
        "truncation_values = df_gsm8k_trunc_wnoise[\"truncation_fraction\"].unique()"
      ],
      "metadata": {
        "id": "t1Xm1_QkD4m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute IQR and filter out outliers\n",
        "Q1 = df_gsm8k_trunc_wnoise[\"percentage_difference\"].quantile(0.25)\n",
        "Q3 = df_gsm8k_trunc_wnoise[\"percentage_difference\"].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define lower and upper bounds to exclude outliers\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Filter outliers to compute the max non-outlier value\n",
        "filtered_df = df_gsm8k_trunc_wnoise[\n",
        "    (df_gsm8k_trunc_wnoise[\"percentage_difference\"] >= lower_bound) &\n",
        "    (df_gsm8k_trunc_wnoise[\"percentage_difference\"] <= upper_bound)\n",
        "]\n",
        "\n",
        "# Compute max percentage difference within non-outliers\n",
        "m = filtered_df[\"percentage_difference\"].max()\n",
        "\n",
        "# Get unique truncation fractions\n",
        "truncation_values = df_gsm8k_trunc_wnoise[\"truncation_fraction\"].unique()\n",
        "\n",
        "# Set up the plot\n",
        "fig, axes = plt.subplots(1, len(truncation_values), figsize=(15, 5), sharey=True)\n",
        "\n",
        "# Plot each scatter plot\n",
        "for i, trunc in enumerate(truncation_values):\n",
        "    subset = df_gsm8k_trunc_wnoise[df_gsm8k_trunc_wnoise[\"truncation_fraction\"] == trunc]\n",
        "    ax = axes[i]\n",
        "    sns.regplot(x=\"noise_factor\", y=\"percentage_difference\", data=subset, ax=ax, scatter=True, ci=None, line_kws={\"color\": \"red\"})\n",
        "    ax.set_title(f\"Truncation Fraction: {trunc}\")\n",
        "    ax.set_xlabel(\"Noise Factor\")\n",
        "    ax.set_ylabel(\"Percentage Difference (%)\")\n",
        "\n",
        "    # Set y-axis limits between -100 and m+100, ensuring valid values\n",
        "    ax.set_ylim(-100, min(m + 100, subset[\"percentage_difference\"].max() + 10))  # Avoid over-stretching\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6eWbUHHJYykd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. CoT faithfullness- White box approach"
      ],
      "metadata": {
        "id": "Hgu2CsMFToG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Stepwise confidence and trends for reasoning paths"
      ],
      "metadata": {
        "id": "bt7znjBqhz4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing stepwise confidence"
      ],
      "metadata": {
        "id": "KcMV4iqPiAw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_reasoning_confidence(logits, gen_tok, tokenizer,inputs):\n",
        "    \"\"\"\n",
        "    Analyze model confidence at each reasoning step in CoT reasoning.\n",
        "\n",
        "    Parameters:\n",
        "    - logits: Model output logits for each token.\n",
        "    - generated_text: Full CoT response as a string.\n",
        "    - tokenizer: Tokenizer used to decode tokens.\n",
        "\n",
        "    Returns:\n",
        "    - List of reasoning steps with their average log probabilities.\n",
        "    \"\"\"\n",
        "    # Convert logits to probabilities\n",
        "\n",
        "    logits = t.stack(logits, dim=1)  # Shape: (batch_size, seq_length, vocab_size)\n",
        "\n",
        "\n",
        "    log_probs = t.nn.functional.log_softmax(logits, dim=-1)\n",
        "    token_probs = log_probs.max(dim=-1).values  # Pick the probability of the chosen token\n",
        "    # print(\"Token_probs_size: \",token_probs.shape)\n",
        "\n",
        "    gen_start = inputs[\"input_ids\"].shape[1]\n",
        "    gen_tokens = gen_tok[:, gen_start:]  # Generated tokens only\n",
        "    # print(\"Full out: \",tokenizer.decode(gen_tok[0]))\n",
        "    # print(\"Generated part: \",tokenizer.decode(gen_tokens[0]))\n",
        "    gen_text_tokens = [tokenizer.decode([tok]) for tok in gen_tokens[0]]  # Decode per token\n",
        "\n",
        "    gen_probs = token_probs\n",
        "    # print(\"Gen_probs shape: \",gen_probs.shape)\n",
        "    # Identify reasoning steps (split by punctuation markers)\n",
        "    step_markers = [\".\", \"?\", \"!\"]\n",
        "    step_indices = [0]\n",
        "\n",
        "    for i, token in enumerate(gen_text_tokens):\n",
        "      if token.endswith(tuple(step_markers)):  # Check if the decoded token ends with punctuation\n",
        "        step_indices.append(i + 1)\n",
        "\n",
        "    # Compute confidence per step\n",
        "    #print(step_indices)\n",
        "    # print(\"Step indices: \",step_indices)\n",
        "    step_confidence = []\n",
        "\n",
        "    for i in range(len(step_indices) - 1):\n",
        "        start, end = step_indices[i], step_indices[i + 1]\n",
        "        if start>=end:\n",
        "          continue\n",
        "\n",
        "        # print(\"Step num \",i+1,\": \",tokenizer.decode(gen_tokens[0,start:end]))\n",
        "        avg_log_prob = gen_probs[0,start:end].mean().item()  # Average confidence\n",
        "\n",
        "        step_confidence.append({\n",
        "            \"step_num\": i+1,\n",
        "            \"avg_log_prob\": avg_log_prob  # More negative → more uncertain\n",
        "        })\n",
        "\n",
        "    return step_confidence"
      ],
      "metadata": {
        "id": "7K04Q3KI-d8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# File paths in Google Drive\n",
        "csv_path_att = \"/content/drive/MyDrive/AI Assignment/MATS- Neel Nanda/Spring 2025/CoT Store/GSM8K/cot_rerun_results.xlsx\"\n",
        "\n",
        "# Check if CSV file exists\n",
        "if os.path.exists(csv_path_att):\n",
        "    print(\"File already exists. Loading previous results...\")\n",
        "\n",
        "    df_gsm8k_att = pd.read_excel(csv_path_att)\n",
        "    print(df_gsm8k_att.head())  # Show first few rows\n",
        "\n",
        "else:\n",
        "    print(\"No existing file found. Running CoT reasoning...\")\n",
        "    df_gsm8k_att,acc = run_cot_with_att(gsm8k_ds_test,csv_path_att,num_runs=1,total=10,with_logits=True,with_att=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "BtSuFE27Vnaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_gsm8k_att = df_gsm8k_att[df_gsm8k_att[\"run_id\"] < 5]"
      ],
      "metadata": {
        "id": "6aHD9EHqgG25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_gsm8k_att.head)"
      ],
      "metadata": {
        "id": "nQb_0sv0gNlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_gsm8k_att[\"conf_per_step\"] = df_gsm8k_att[\"conf_per_step\"].apply(ast.literal_eval)"
      ],
      "metadata": {
        "id": "4M13vOe1pPfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting stepwise confidence for all CoTs"
      ],
      "metadata": {
        "id": "T7eGwZnyiI3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_multiple_confidence_trends(df):\n",
        "    plt.figure(figsize=(10, 6))  # Set figure size\n",
        "\n",
        "    all_step_nums = []\n",
        "    all_avg_log_probs = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        conf_per_step = row[\"conf_per_step\"]  # Extract step confidence data\n",
        "\n",
        "        # Extract step numbers and log probabilities\n",
        "        step_nums = [entry[\"step_num\"] for entry in conf_per_step]\n",
        "        avg_log_probs = [entry[\"avg_log_prob\"] for entry in conf_per_step]\n",
        "\n",
        "        # Convert to NumPy array to handle NaN values\n",
        "        step_nums = np.array(step_nums)\n",
        "        avg_log_probs = np.array(avg_log_probs)\n",
        "\n",
        "        # Mask NaN values to prevent breaks in the plot\n",
        "        valid_mask = ~np.isnan(avg_log_probs)\n",
        "        plt.plot(step_nums[valid_mask], avg_log_probs[valid_mask],\n",
        "                 marker=\"o\", linestyle=\"-\", color=\"dimgray\", alpha=0.5)  # Darker gray\n",
        "\n",
        "        # Store data for trend line\n",
        "        all_step_nums.extend(step_nums[valid_mask])\n",
        "        all_avg_log_probs.extend(avg_log_probs[valid_mask])\n",
        "\n",
        "    # Compute trend line (average confidence per step)\n",
        "    if all_step_nums:\n",
        "        unique_steps = sorted(set(all_step_nums))  # Get unique step numbers\n",
        "        avg_trend = [np.mean([all_avg_log_probs[i] for i in range(len(all_step_nums)) if all_step_nums[i] == step])\n",
        "                     for step in unique_steps]\n",
        "\n",
        "        # Plot overall trend line in orange\n",
        "        plt.plot(unique_steps, avg_trend, color=\"orange\", linestyle=\"-\", linewidth=2, label=\"Average Trend\")\n",
        "\n",
        "    plt.xlabel(\"Reasoning Step Number\")\n",
        "    plt.ylabel(\"Average Log Probability\")\n",
        "    plt.title(\"Confidence Trends Across Different CoTs\")\n",
        "    plt.axhline(y=0, color=\"black\", linestyle=\"--\", label=\"Baseline (log_prob = 0)\")\n",
        "\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Call function with dataframe\n",
        "plot_multiple_confidence_trends(df_gsm8k_att)\n",
        "\n"
      ],
      "metadata": {
        "id": "9uN5p6xfZF86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification of confidence trends- K Means"
      ],
      "metadata": {
        "id": "JAKv4hKIiOW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tslearn\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tslearn.clustering import TimeSeriesKMeans\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xp0hyHh2o1nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
        "from tslearn.clustering import TimeSeriesKMeans\n",
        "\n",
        "# Extract valid sequences while handling NaNs\n",
        "valid_indices = []\n",
        "valid_sequences = []\n",
        "\n",
        "for idx, seq in enumerate(df_gsm8k_att[\"conf_per_step\"]):\n",
        "    log_probs = np.array([step[\"avg_log_prob\"] for step in seq if not np.isnan(step[\"avg_log_prob\"])])\n",
        "    if len(log_probs) > 0:  # Keep only non-empty sequences\n",
        "        valid_sequences.append(log_probs)\n",
        "        valid_indices.append(idx)\n",
        "\n",
        "# Check if we have valid sequences\n",
        "if not valid_sequences:\n",
        "    print(\"No valid sequences found. Skipping clustering.\")\n",
        "else:\n",
        "    # Normalize sequence lengths by padding to max length\n",
        "    max_len = max(len(seq) for seq in valid_sequences)\n",
        "    sequences_padded = np.array([\n",
        "        np.pad(seq, (0, max_len - len(seq)), 'constant', constant_values=np.nan)\n",
        "        for seq in valid_sequences\n",
        "    ])\n",
        "\n",
        "    # Normalize using mean variance scaling\n",
        "    scaler = TimeSeriesScalerMeanVariance()\n",
        "    X_dtw = scaler.fit_transform(sequences_padded)\n",
        "\n",
        "    # Apply DTW K-Means Clustering\n",
        "    n_clusters = 6  # Adjust as needed\n",
        "    dtw_km = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"dtw\", verbose=True, random_state=42)\n",
        "    cluster_labels = dtw_km.fit_predict(X_dtw)\n",
        "\n",
        "    # Store results in the original DataFrame\n",
        "    df_gsm8k_att[\"dtw_cluster\"] = np.nan  # Initialize with NaN\n",
        "    df_gsm8k_att.loc[valid_indices, \"dtw_cluster\"] = cluster_labels\n",
        "\n",
        "    cluster_representative_cots = {}\n",
        "\n",
        "    for cluster in sorted(df_gsm8k_att[\"dtw_cluster\"].dropna().unique()):  # Get unique clusters\n",
        "      # Get all indices of rows belonging to this cluster\n",
        "      cluster_indices = df_gsm8k_att[df_gsm8k_att[\"dtw_cluster\"] == cluster].index.tolist()\n",
        "\n",
        "      # Pick up to three representative CoTs (if available)\n",
        "      representative_indices = cluster_indices[:3]  # Take the first three\n",
        "      cluster_representative_cots[cluster] = df_gsm8k_att.loc[representative_indices, \"cot_generated\"].tolist()\n",
        "\n",
        "    # Print or save the representative CoTs\n",
        "    for cluster, cots in cluster_representative_cots.items():\n",
        "      print(f\"Cluster {cluster}:\")\n",
        "      for i, cot in enumerate(cots, 1):\n",
        "        print(f\"  Example {i}: {cot}\\n\")\n",
        "\n",
        "# Display dataframe with assigned clusters\n",
        "print(df_gsm8k_att[[\"conf_per_step\", \"dtw_cluster\", \"cot_generated\"]])\n"
      ],
      "metadata": {
        "id": "cQWiBkJqhXGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_cluster_confidence_trends(df, cluster_representative_cots):\n",
        "    unique_clusters = sorted(df[\"dtw_cluster\"].dropna().unique())  # Get unique cluster IDs\n",
        "\n",
        "    for cluster in unique_clusters:\n",
        "        plt.figure(figsize=(8, 6))  # Separate figure for each cluster\n",
        "\n",
        "        # Subset data for the cluster\n",
        "        cluster_df = df[df[\"dtw_cluster\"] == cluster]\n",
        "\n",
        "        all_step_nums = []\n",
        "        all_avg_log_probs = []\n",
        "\n",
        "        for _, row in cluster_df.iterrows():\n",
        "            conf_per_step = row[\"conf_per_step\"]  # Extract confidence steps\n",
        "\n",
        "            # Extract step numbers and log probabilities\n",
        "            step_nums = [entry[\"step_num\"] for entry in conf_per_step]\n",
        "            avg_log_probs = [entry[\"avg_log_prob\"] for entry in conf_per_step]\n",
        "\n",
        "            # Convert to NumPy array to handle NaNs\n",
        "            step_nums = np.array(step_nums)\n",
        "            avg_log_probs = np.array(avg_log_probs)\n",
        "\n",
        "            # Mask NaN values\n",
        "            valid_mask = ~np.isnan(avg_log_probs)\n",
        "            plt.plot(step_nums[valid_mask], avg_log_probs[valid_mask],\n",
        "                     marker=\"o\", linestyle=\"-\", color=\"dimgray\", alpha=0.5)  # Dark grey lines\n",
        "\n",
        "            # Store for trend line\n",
        "            all_step_nums.extend(step_nums[valid_mask])\n",
        "            all_avg_log_probs.extend(avg_log_probs[valid_mask])\n",
        "\n",
        "        # Compute trend line (average log probability per step in cluster)\n",
        "        if all_step_nums:\n",
        "            unique_steps = sorted(set(all_step_nums))  # Get unique step numbers\n",
        "            avg_trend = [np.mean([all_avg_log_probs[i] for i in range(len(all_step_nums)) if all_step_nums[i] == step])\n",
        "                         for step in unique_steps]\n",
        "\n",
        "            # Plot overall trend line in orange\n",
        "            plt.plot(unique_steps, avg_trend, color=\"orange\", linestyle=\"-\", linewidth=2, label=\"Avg Trend\")\n",
        "\n",
        "        plt.xlabel(\"Reasoning Step Number\")\n",
        "        plt.ylabel(\"Average Log Probability\")\n",
        "        plt.title(f\"Confidence Trends for DTW Cluster {cluster}\")\n",
        "        plt.axhline(y=0, color=\"black\", linestyle=\"--\", label=\"Baseline (log_prob = 0)\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Show plot\n",
        "        plt.show()\n",
        "\n",
        "        # Display the three representative CoTs for this cluster\n",
        "        print(f\"\\n=== Representative CoTs for Cluster {cluster} ===\\n\")\n",
        "        representative_cots = cluster_representative_cots.get(cluster, [])\n",
        "\n",
        "        if representative_cots:\n",
        "            for i, cot in enumerate(representative_cots, 1):\n",
        "                print(f\"Example {i}:\\n{cot}\\n\")\n",
        "        else:\n",
        "            print(\"No representative CoTs found.\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "\n",
        "# Call function with clustering results\n",
        "plot_cluster_confidence_trends(df_gsm8k_att, cluster_representative_cots)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AwwVUAqpheiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification of confidence trends- With HDB Scan"
      ],
      "metadata": {
        "id": "NAkn4HD9mNPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tslearn hdbscan\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import hdbscan\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
        "from tslearn.metrics import cdist_dtw  # DTW distance computation\n",
        "\n",
        "# Sample Data (Replace with df_gsm8k_att[\"conf_per_step\"])\n",
        "df = pd.DataFrame({\n",
        "    \"conf_per_step\": [\n",
        "        [{'step_num': 0, 'avg_log_prob': -0.22}, {'step_num': 1, 'avg_log_prob': -0.16}, {'step_num': 2, 'avg_log_prob': -0.1}],\n",
        "        [{'step_num': 0, 'avg_log_prob': -0.5}, {'step_num': 1, 'avg_log_prob': -0.3}, {'step_num': 2, 'avg_log_prob': -0.1}],\n",
        "        [{'step_num': 0, 'avg_log_prob': -0.2}, {'step_num': 1, 'avg_log_prob': -0.25}, {'step_num': 2, 'avg_log_prob': -0.5}],\n",
        "        [{'step_num': 0, 'avg_log_prob': -0.1}, {'step_num': 1, 'avg_log_prob': -0.05}, {'step_num': 2, 'avg_log_prob': -0.02}],\n",
        "        [{'step_num': 0, 'avg_log_prob': -0.3}, {'step_num': 1, 'avg_log_prob': -0.28}, {'step_num': 2, 'avg_log_prob': -0.1}],\n",
        "        [{'step_num': 0, 'avg_log_prob': -0.56}, {'step_num': 1, 'avg_log_prob': -0.1}, {'step_num': 2, 'avg_log_prob': -0.45},{'step_num': 3, 'avg_log_prob': -0.05}],\n",
        "        [{'step_num': 0, 'avg_log_prob': -0.75}, {'step_num': 1, 'avg_log_prob': -0.5}, {'step_num': 2, 'avg_log_prob': -0.25},{'step_num': 3, 'avg_log_prob': -0.05}]\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Extract log probability sequences & handle NaNs\n",
        "sequences = [\n",
        "    np.array([step[\"avg_log_prob\"] for step in seq if not np.isnan(step[\"avg_log_prob\"])])\n",
        "    for seq in df[\"conf_per_step\"]\n",
        "]\n",
        "\n",
        "# Normalize sequence lengths by padding to max length\n",
        "max_len = max(len(seq) for seq in sequences)\n",
        "sequences_padded = np.array([\n",
        "    np.pad(seq, (0, max_len - len(seq)), 'constant', constant_values=np.nan)\n",
        "    for seq in sequences\n",
        "])\n",
        "\n",
        "# Normalize sequences using mean variance scaling\n",
        "scaler = TimeSeriesScalerMeanVariance()\n",
        "X_scaled = scaler.fit_transform(sequences_padded)\n",
        "\n",
        "# Compute DTW distance matrix\n",
        "dtw_distance_matrix = cdist_dtw(X_scaled)\n",
        "\n",
        "# Apply HDBSCAN clustering (metric=\"precomputed\" since we use a DTW distance matrix)\n",
        "hdb = hdbscan.HDBSCAN(min_cluster_size=2, metric=\"precomputed\")\n",
        "df[\"hdbscan_cluster\"] = hdb.fit_predict(dtw_distance_matrix)\n",
        "\n",
        "# Print cluster assignments\n",
        "print(df[[\"conf_per_step\", \"hdbscan_cluster\"]])\n"
      ],
      "metadata": {
        "id": "CeJW5T-imRq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_clusters = df[\"hdbscan_cluster\"].nunique()\n",
        "print(f\"Number of clusters found: {num_clusters}\")"
      ],
      "metadata": {
        "id": "ZnoXHMBSnEie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Analysing attention patterns"
      ],
      "metadata": {
        "id": "cGnoZ2Rnp6v9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing backtracking ratios"
      ],
      "metadata": {
        "id": "RdGzcjFwyZt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_stepwise_backtracking_ratio(attentions, gen_tok, tokenizer, inputs, k=2):\n",
        "    \"\"\"\n",
        "    Compute the average backtracking ratio for each reasoning step and overall average.\n",
        "\n",
        "    Parameters:\n",
        "    - attentions: Tensor of shape (num_layers, num_heads, seq_len, seq_len)\n",
        "    - gen_tokens: Tensor of generated token IDs (batch_size, seq_len)\n",
        "    - tokenizer: Tokenizer to decode tokens\n",
        "    - k: Number of reasoning steps back to consider as \"far past\"\n",
        "\n",
        "    Returns:\n",
        "    - avg_ratios_per_step: List of average backtracking ratios per step.\n",
        "    - overall_avg_ratio: Overall average backtracking ratio across all steps.\n",
        "    \"\"\"\n",
        "\n",
        "    # num_layers, num_heads, seq_len, _ = attentions[0].shape\n",
        "    num_layers = len(attentions[0])\n",
        "    num_heads = attentions[0][0].shape[1]\n",
        "    # seq_len = len(attentions)\n",
        "    avg_ratios_per_step = {}\n",
        "\n",
        "    # Decode generated tokens to identify reasoning steps\n",
        "    gen_start = inputs[\"input_ids\"].shape[1]\n",
        "    gen_tokens = gen_tok[:, gen_start:]  # Generated tokens only\n",
        "    gen_text_tokens = [tokenizer.decode([tok]) for tok in gen_tokens[0]]\n",
        "    step_markers = [\".\", \"?\", \"!\"]\n",
        "    step_indices = [0]\n",
        "\n",
        "    # Identify step boundaries\n",
        "    for i, token in enumerate(gen_text_tokens):\n",
        "        if token.endswith(tuple(step_markers)):\n",
        "            step_indices.append(i + 1)\n",
        "\n",
        "    # print(len(attentions))\n",
        "    count =0\n",
        "\n",
        "    # Iterate over steps\n",
        "    if len(step_indices)-1 < 3:\n",
        "      avg_ratios_per_step= None\n",
        "      overall_avg_ratio = None\n",
        "\n",
        "    else:\n",
        "      for i in range(len(step_indices) - 1):\n",
        "        count +=1\n",
        "        # print(\"Count :\",count)\n",
        "        start, end = step_indices[i], step_indices[i + 1]\n",
        "        if start >= end:\n",
        "          continue\n",
        "\n",
        "        step_attention_ratios = []  # Store ratios for all tokens in the step\n",
        "        # print(gen_text_tokens[start:end])\n",
        "\n",
        "        for to in range(start, end):\n",
        "            token_ratios = []  # Store backtracking ratio for all layers/heads at token t\n",
        "            temp = attentions[to]\n",
        "\n",
        "\n",
        "            for layer in range(num_layers):\n",
        "                attention = temp[layer]\n",
        "                for head in range(num_heads):\n",
        "                    attn_matrix = attention[0,head]  # Shape (seq_len, seq_len)\n",
        "\n",
        "                    # Sum of attention to all past tokens\n",
        "                    past_attention = attn_matrix[0, :to].sum()\n",
        "\n",
        "                    if i>1:\n",
        "                      # Sum of attention to the first 25% of tokens in the series\n",
        "                      far_past_start = step_indices[max(0, i // 4)]\n",
        "                      far_past_attention = attn_matrix[0,:far_past_start].sum()\n",
        "\n",
        "                      # Compute ratio (avoid division by zero)\n",
        "                      ratio = (far_past_attention / past_attention).item() if past_attention > 0 else 0\n",
        "                      token_ratios.append(ratio)\n",
        "\n",
        "            # Average ratio across all layers and heads for this token\n",
        "            step_attention_ratios.append(t.tensor(token_ratios, dtype=t.float32).mean().item())\n",
        "\n",
        "        # Average across all tokens in the step\n",
        "        avg_ratios_per_step[i]=t.tensor(step_attention_ratios).mean().item()\n",
        "\n",
        "    return avg_ratios_per_step"
      ],
      "metadata": {
        "id": "c1VZ6PnmCrsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# File paths in Google Drive\n",
        "csv_path_atts = \"/content/drive/MyDrive/AI Assignment/MATS- Neel Nanda/Spring 2025/CoT Store/GSM8K/cot_rerun_results.xlsx\"\n",
        "\n",
        "# Check if CSV file exists\n",
        "if os.path.exists(csv_path_atts):\n",
        "    print(\"File already exists. Loading previous results...\")\n",
        "\n",
        "    df_gsm8k_atts = pd.read_excel(csv_path_atts)\n",
        "    print(df_gsm8k_atts.head())  # Show first few rows\n",
        "\n",
        "else:\n",
        "    print(\"No existing file found. Running CoT reasoning...\")\n",
        "    df_gsm8k_atts,acc = run_cot_with_att(gsm8k_ds_test,csv_path_atts,num_runs=1,total=10,with_logits=False,with_att=True)"
      ],
      "metadata": {
        "id": "0YIWLAV1exZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_gsm8k_atts = df_gsm8k_atts[df_gsm8k_atts[\"run_id\"] < 5]"
      ],
      "metadata": {
        "id": "DyGvcsKzu21c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_gsm8k_atts[\"bratio_per_step\"].dropna"
      ],
      "metadata": {
        "id": "eLTP2tuzj4hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_gsm8k_atts[\"bratio_per_step\"]"
      ],
      "metadata": {
        "id": "fHJFm6xdkNVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_bratio_per_step(value):\n",
        "    if not isinstance(value, str) or value.lower() == \"nan\":\n",
        "        return []  # Ignore NaNs\n",
        "\n",
        "    # Extract key-value pairs (numbers before and after ':')\n",
        "    matches = re.findall(r\"(\\d+):\\s*([\\d\\.\\-e]+)\", value)  # Supports float & scientific notation\n",
        "\n",
        "    parsed_values = [(int(k), float(v)) for k, v in matches if v.lower() != \"nan\"]\n",
        "\n",
        "    return parsed_values  # Returns list of tuples [(x1, y1), (x2, y2), ...]\n",
        "\n",
        "# Apply parsing function\n",
        "df_gsm8k_atts[\"bratio_per_step_parsed\"] = df_gsm8k_atts[\"bratio_per_step\"].apply(parse_bratio_per_step)\n",
        "\n",
        "# Print first 10 parsed values\n",
        "print(df_gsm8k_atts[\"bratio_per_step_parsed\"].head(10).tolist())\n"
      ],
      "metadata": {
        "id": "XXUk58rYlV3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting backtracking ratios for all CoTs"
      ],
      "metadata": {
        "id": "bTHMHpm1yhAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_backtracking_ratio(df):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    all_step_nums = []\n",
        "    all_ratios = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        parsed_values = row[\"bratio_per_step_parsed\"]\n",
        "\n",
        "        if parsed_values:  # Skip empty rows\n",
        "            x_values, y_values = zip(*parsed_values)  # Unzip into X and Y\n",
        "\n",
        "            # Convert to NumPy array to handle NaNs\n",
        "            x_values = np.array(x_values)\n",
        "            y_values = np.array(y_values)\n",
        "\n",
        "            # Mask NaN values\n",
        "            valid_mask = ~np.isnan(y_values)\n",
        "            plt.plot(x_values[valid_mask], y_values[valid_mask],\n",
        "                     marker='o', linestyle='-', color=\"dimgray\", alpha=0.5)  # Dark grey lines\n",
        "\n",
        "            # Store values for average trend calculation\n",
        "            all_step_nums.extend(x_values[valid_mask])\n",
        "            all_ratios.extend(y_values[valid_mask])\n",
        "\n",
        "    # Compute trend line (average backtracking ratio per step)\n",
        "    if all_step_nums:\n",
        "        unique_steps = sorted(set(all_step_nums))  # Get unique step numbers\n",
        "        avg_trend = [np.mean([all_ratios[i] for i in range(len(all_step_nums)) if all_step_nums[i] == step])\n",
        "                     for step in unique_steps]\n",
        "\n",
        "        # Plot overall trend line in orange\n",
        "        plt.plot(unique_steps, avg_trend, color=\"orange\", linestyle=\"-\", linewidth=2, label=\"Avg Trend\")\n",
        "\n",
        "    # Labels and title\n",
        "    plt.xlabel(\"Step Index\")\n",
        "    plt.ylabel(\"Backtracking Ratio\")\n",
        "    plt.title(\"Backtracking Ratio per Step\")\n",
        "    plt.axhline(y=0, color=\"black\", linestyle=\"--\", label=\"Baseline (Ratio = 0)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Call the function\n",
        "plot_backtracking_ratio(df_gsm8k_atts)\n",
        "\n"
      ],
      "metadata": {
        "id": "YpSY3LoMldY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification of backtracking rations - K means"
      ],
      "metadata": {
        "id": "Xy5IRukpymLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
        "from tslearn.clustering import TimeSeriesKMeans\n",
        "\n",
        "def perform_kmeans_clustering(df, n_clusters=3):\n",
        "    # Extract sequences of backtracking ratios\n",
        "    sequences = [\n",
        "        np.array([step[1] for step in row[\"bratio_per_step_parsed\"]])\n",
        "        for _, row in df.iterrows() if row[\"bratio_per_step_parsed\"]\n",
        "    ]\n",
        "\n",
        "    # Normalize sequence lengths by padding to max length\n",
        "    max_len = max(len(seq) for seq in sequences)\n",
        "    sequences_padded = np.array([\n",
        "        np.pad(seq, (0, max_len - len(seq)), 'constant', constant_values=np.nan)\n",
        "        for seq in sequences\n",
        "    ])\n",
        "\n",
        "    # Normalize sequences\n",
        "    scaler = TimeSeriesScalerMeanVariance()\n",
        "    X_scaled = scaler.fit_transform(sequences_padded)\n",
        "\n",
        "    # Apply K-Means clustering (using DTW as the metric)\n",
        "    kmeans = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"dtw\", random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "    # Assign cluster labels to DataFrame\n",
        "    df[\"kmeans_cluster\"] = np.nan  # Initialize with NaN\n",
        "    df.loc[df.index[:len(cluster_labels)], \"kmeans_cluster\"] = cluster_labels\n",
        "\n",
        "    return df, cluster_labels\n",
        "\n",
        "def get_representative_cots(df, n_clusters=3):\n",
        "    cluster_representative_cots = {}\n",
        "\n",
        "    for cluster in range(n_clusters):\n",
        "        cluster_df = df[df[\"kmeans_cluster\"] == cluster]\n",
        "        if not cluster_df.empty:\n",
        "            # Select 3 representative CoTs from the cluster\n",
        "            cluster_representative_cots[cluster] = cluster_df[\"cot_generated\"].head(3).tolist()\n",
        "\n",
        "    return cluster_representative_cots\n",
        "\n",
        "def plot_cluster_trends(df, cluster_representative_cots, n_clusters=3):\n",
        "    for cluster in range(n_clusters):\n",
        "        plt.figure(figsize=(8, 6))\n",
        "\n",
        "        # Subset data for the cluster\n",
        "        cluster_df = df[df[\"kmeans_cluster\"] == cluster]\n",
        "\n",
        "        all_step_nums = []\n",
        "        all_ratios = []\n",
        "\n",
        "        for _, row in cluster_df.iterrows():\n",
        "            parsed_values = row[\"bratio_per_step_parsed\"]\n",
        "\n",
        "            if parsed_values:\n",
        "                x_values, y_values = zip(*parsed_values)\n",
        "\n",
        "                # Convert to NumPy array to handle NaNs\n",
        "                x_values = np.array(x_values)\n",
        "                y_values = np.array(y_values)\n",
        "\n",
        "                # Mask NaN values\n",
        "                valid_mask = ~np.isnan(y_values)\n",
        "                plt.plot(x_values[valid_mask], y_values[valid_mask],\n",
        "                         marker='o', linestyle='-', color=\"dimgray\", alpha=0.5)\n",
        "\n",
        "                # Store for trend line\n",
        "                all_step_nums.extend(x_values[valid_mask])\n",
        "                all_ratios.extend(y_values[valid_mask])\n",
        "\n",
        "        # Compute and plot the average trend line\n",
        "        if all_step_nums:\n",
        "            unique_steps = sorted(set(all_step_nums))\n",
        "            avg_trend = [np.mean([all_ratios[i] for i in range(len(all_step_nums)) if all_step_nums[i] == step])\n",
        "                         for step in unique_steps]\n",
        "            plt.plot(unique_steps, avg_trend, color=\"orange\", linestyle=\"-\", linewidth=2, label=\"Avg Trend\")\n",
        "\n",
        "        plt.xlabel(\"Step Index\")\n",
        "        plt.ylabel(\"Backtracking Ratio\")\n",
        "        plt.title(f\"Backtracking Ratio Trends for Cluster {cluster}\")\n",
        "        plt.axhline(y=0, color=\"black\", linestyle=\"--\", label=\"Baseline (Ratio = 0)\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Show plot\n",
        "        plt.show()\n",
        "\n",
        "        # Print 3 representative CoTs for this cluster\n",
        "        print(f\"\\n=== Typical CoTs for Cluster {cluster} ===\\n\")\n",
        "        for i, cot in enumerate(cluster_representative_cots.get(cluster, [])):\n",
        "            print(f\"Example {i+1}:\\n{cot}\\n\")\n",
        "        print(\"=\" * 50 + \"\\n\")\n",
        "\n",
        "# Perform clustering and save cluster labels\n",
        "df_gsm8k_atts, cluster_labels = perform_kmeans_clustering(df_gsm8k_atts, n_clusters=3)\n",
        "\n",
        "# Get 3 representative CoTs per cluster\n",
        "cluster_representative_cots = get_representative_cots(df_gsm8k_atts, n_clusters=3)\n",
        "\n",
        "# Plot trends for each cluster with representative CoTs\n",
        "plot_cluster_trends(df_gsm8k_atts, cluster_representative_cots, n_clusters=3)\n"
      ],
      "metadata": {
        "id": "HkNXK9HmzRtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Reasoning steering"
      ],
      "metadata": {
        "id": "wZFco60ggjK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generated synthetic math dataset"
      ],
      "metadata": {
        "id": "rg3kdpd-5IY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "def generate_math_dataset(num_examples=100):\n",
        "    dataset = []\n",
        "    operations = [\"+\", \"-\", \"*\", \"/\", \"**\", \"sqrt\", \"lcm\", \"gcd\", \"!\"]\n",
        "\n",
        "    for _ in range(num_examples):\n",
        "        op = random.choice(operations)\n",
        "\n",
        "        # Generate numbers based on operation\n",
        "        if op in [\"+\", \"-\", \"*\", \"/\"]:\n",
        "            a, b = random.randint(10, 99), random.randint(10, 99)\n",
        "            if op == \"/\":\n",
        "                b = random.randint(1, 10)\n",
        "\n",
        "        elif op == \"**\":\n",
        "            a = random.randint(2, 9)\n",
        "            b = random.randint(2, 4)\n",
        "\n",
        "        elif op == \"sqrt\":\n",
        "            a = random.randint(4, 100)\n",
        "            a = a * a  # Ensure perfect squares\n",
        "            b = None\n",
        "\n",
        "        elif op in [\"lcm\", \"gcd\"]:\n",
        "            a, b = random.randint(10, 50), random.randint(10, 50)\n",
        "\n",
        "        elif op == \"!\":\n",
        "            a = random.randint(3, 7)\n",
        "            b = None\n",
        "\n",
        "        # Compute correct result\n",
        "        if op == \"+\":\n",
        "            correct_answer = a + b\n",
        "        elif op == \"-\":\n",
        "            correct_answer = a - b\n",
        "        elif op == \"*\":\n",
        "            correct_answer = a * b\n",
        "        elif op == \"/\":\n",
        "            correct_answer = round(a / b, 2)\n",
        "        elif op == \"**\":\n",
        "            correct_answer = a ** b\n",
        "        elif op == \"sqrt\":\n",
        "            correct_answer = int(math.sqrt(a))\n",
        "        elif op == \"lcm\":\n",
        "            correct_answer = math.lcm(a, b)\n",
        "        elif op == \"gcd\":\n",
        "            correct_answer = math.gcd(a, b)\n",
        "        elif op == \"!\":\n",
        "            correct_answer = math.factorial(a)\n",
        "\n",
        "        # 50% chance of wrong answer\n",
        "        is_wrong = random.choice([True, False])\n",
        "\n",
        "        if is_wrong:\n",
        "            # Generate a confident wrong answer\n",
        "            if isinstance(correct_answer, int):\n",
        "                wrong_answer = correct_answer + random.choice([-3, -2, -1, 1, 2, 3])\n",
        "            else:\n",
        "                wrong_answer = round(correct_answer + random.choice([-0.5, -0.3, 0.3, 0.5]), 2)\n",
        "        else:\n",
        "            wrong_answer = correct_answer  # No mistake\n",
        "\n",
        "        # Format the question text\n",
        "        if op in [\"+\", \"-\", \"*\", \"/\"]:\n",
        "            input_text = f\"What is {a} {op} {b}?\"\n",
        "        elif op == \"**\":\n",
        "            input_text = f\"What is {a} raised to the power of {b}?\"\n",
        "        elif op == \"sqrt\":\n",
        "            input_text = f\"What is the square root of {a}?\"\n",
        "        elif op == \"lcm\":\n",
        "            input_text = f\"What is the least common multiple of {a} and {b}?\"\n",
        "        elif op == \"gcd\":\n",
        "            input_text = f\"What is the greatest common divisor of {a} and {b}?\"\n",
        "        elif op == \"!\":\n",
        "            input_text = f\"What is {a} factorial?\"\n",
        "\n",
        "        # Generate first try response (forward calculation)\n",
        "        if op in [\"+\", \"-\", \"*\", \"/\"]:\n",
        "            negative_output = f\"So, {a}{op}{b} = {wrong_answer}.\"\n",
        "        elif op == \"**\":\n",
        "            negative_output = f\"So, {a}^({b}) = {wrong_answer}.\"\n",
        "        elif op == \"sqrt\":\n",
        "            negative_output = f\"So, sqrt({a}) = {wrong_answer}.\"\n",
        "        elif op == \"lcm\":\n",
        "            negative_output = f\"So, LCM({a}, {b}) = {wrong_answer}.\"\n",
        "        elif op == \"gcd\":\n",
        "            negative_output = f\"So, GCD({a}, {b}) = {wrong_answer}.\"\n",
        "        elif op == \"!\":\n",
        "            negative_output = f\"So, {a}! = {wrong_answer}.\"\n",
        "\n",
        "        # Generate rechecking step (inverse calculation)\n",
        "        if is_wrong:\n",
        "            if op == \"+\":\n",
        "                recheck_output = f\"{negative_output} Let me check again... {b} + {a} = {correct_answer}.\"\n",
        "            elif op == \"-\":\n",
        "              if not(is_wrong):\n",
        "                recheck_output = f\"{negative_output} Let me check again... {wrong_answer} + {b} = {a}.\"\n",
        "              else:\n",
        "                recheck_output = f\"{negative_output} Let me check again... {wrong_answer} + {b} = {wrong_answer+b} which is not equal to {a}. Hence the first computation was wrong. Computing again {a} + {b} = {correct_answer}\"\n",
        "            elif op == \"*\":\n",
        "                recheck_output = f\"{negative_output} Let me check again... {b} * {a} = {correct_answer}.\"\n",
        "            elif op == \"/\":\n",
        "              if not(is_wrong):\n",
        "                recheck_output = f\"{negative_output} Let me check again... {correct_answer} * {b} = {a}.\"\n",
        "              else:\n",
        "                recheck_output = f\"{negative_output} Let me check again... {wrong_answer} * {b} = {b*wrong_answer} which is not equal to {a}. Hence the first computation was wrong. Computing again {a} / {b} = {correct_answer}\"\n",
        "            elif op == \"**\":\n",
        "              if not(is_wrong):\n",
        "                recheck_output = f\"{negative_output} Let me check again... The {b}th root of {correct_answer} is {a}.\"\n",
        "              else:\n",
        "                recheck_output = f\"{negative_output} Let me check again... The {b}th root of {wrong_answer} is {wrong_answer ** (1/b)} which is not equal to{a}. Hence the first computation was wrong. Computing again {a} ** {b} = {correct_answer}\"\n",
        "            elif op == \"sqrt\":\n",
        "              if not(is_wrong):\n",
        "                recheck_output = f\"{negative_output} Let me check again... {correct_answer}^2 = {a}.\"\n",
        "              else:\n",
        "                recheck_output = f\"{negative_output} Let me check again... {wrong_answer}^2 = {wrong_answer^2} which is not equal to {a}. Hence the first computation was wrong. Computing again sqrt({a}) = {correct_answer}\"\n",
        "            elif op == \"lcm\":\n",
        "                recheck_output = f\"{negative_output} Let me check again... The smallest multiple of both {a} and {b} is {correct_answer}.\"\n",
        "            elif op == \"gcd\":\n",
        "                recheck_output = f\"{negative_output} Let me check again... The largest number that divides both {a} and {b} is {correct_answer}.\"\n",
        "            elif op == \"!\":\n",
        "                recheck_output = f\"{negative_output} Let me check again... {a-1}! * {a} = {correct_answer}.\"\n",
        "        else:\n",
        "            recheck_output = negative_output  # No need to recheck if already correct\n",
        "\n",
        "        # Store in dataset\n",
        "        dataset.append((input_text, recheck_output, negative_output))\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def save_math_dataset(csv_path, num_examples=100):\n",
        "    dataset = generate_math_dataset(num_examples)\n",
        "    df = pd.DataFrame(dataset, columns=[\"Question\", \"Positive Response\", \"Negative Response\"])\n",
        "    print(df.head)\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"Dataset saved at: {csv_path}\")\n",
        "\n",
        "# Example usage\n",
        "csv_path = \"/content/drive/MyDrive/AI Assignment/MATS- Neel Nanda/Spring 2025/CoT Store/Math_Dset.csv\"\n",
        "save_math_dataset(csv_path, num_examples=100)\n"
      ],
      "metadata": {
        "id": "HhY2hbNsuH3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute steering vector"
      ],
      "metadata": {
        "id": "MC0GeNPR6EWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_per_token_activations(text, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Extracts per-token activations for all layers.\n",
        "\n",
        "    Returns:\n",
        "    - Dictionary where keys are layer indices and values are (seq_len, hidden_dim) numpy arrays.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with t.no_grad():\n",
        "        outputs = model(**inputs, output_hidden_states=True)\n",
        "\n",
        "    # Extract hidden states for all layers\n",
        "    all_layer_hidden_states = outputs.hidden_states  # Tuple of (num_layers + 1, batch_size, seq_len, hidden_dim)\n",
        "\n",
        "    # Convert to dictionary with layers as keys\n",
        "    activations = {\n",
        "        layer_idx: layer_states.squeeze(0).cpu().numpy()  # Shape: (seq_len, hidden_dim)\n",
        "        for layer_idx, layer_states in enumerate(all_layer_hidden_states)\n",
        "    }\n",
        "\n",
        "    return activations  # Dict[layer] -> (seq_len, hidden_dim)"
      ],
      "metadata": {
        "id": "wXeUZ6rd50DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_steering_vector(dataset, model, tokenizer):\n",
        "    \"\"\"Computes the steering vector using activations from all layers and identifies the most affected layer.\"\"\"\n",
        "\n",
        "    num_layers = model.config.num_hidden_layers + 1  # +1 to include input embeddings\n",
        "    layer_norm_changes = np.zeros(num_layers)  # Track activation change per layer\n",
        "\n",
        "    # Store steering vectors and activations\n",
        "    steering_vectors = {layer: [] for layer in range(num_layers)}\n",
        "    activations_per_layer = {layer: {\"wrong\": [], \"self_check\": [], \"corrected\": []} for layer in range(num_layers)}\n",
        "\n",
        "    for question, recheck_response, wrong_response in tqdm(dataset):\n",
        "        # Get per-token activations for both responses\n",
        "        wrong_acts = get_per_token_activations(wrong_response, model, tokenizer)\n",
        "        recheck_acts = get_per_token_activations(recheck_response, model, tokenizer)\n",
        "\n",
        "        # Tokenize to find key tokens for activation extraction\n",
        "        wrong_tokens = tokenizer.tokenize(wrong_response)\n",
        "        recheck_tokens = tokenizer.tokenize(recheck_response)\n",
        "\n",
        "        # Identify token positions (last token of each stage)\n",
        "        idx_wrong = len(wrong_tokens) - 2  # Last token before the period\n",
        "        idx_self_check = recheck_tokens.index(\"Let\") if \"Let\" in recheck_tokens else idx_wrong\n",
        "        idx_corrected = len(recheck_tokens) - 2  # Last token before final period\n",
        "\n",
        "        # Compute steering vectors per layer\n",
        "        for layer in range(num_layers):\n",
        "            act_wrong = wrong_acts[layer][idx_wrong]  # Activation at wrong answer\n",
        "            act_self_check = recheck_acts[layer][idx_self_check]  # Activation at self-verification step\n",
        "            act_corrected = recheck_acts[layer][idx_corrected]  # Activation at corrected answer\n",
        "\n",
        "            # Store raw activations for later analysis\n",
        "            activations_per_layer[layer][\"wrong\"].append(act_wrong)\n",
        "            activations_per_layer[layer][\"self_check\"].append(act_self_check)\n",
        "            activations_per_layer[layer][\"corrected\"].append(act_corrected)\n",
        "\n",
        "            # Compute different steering vectors\n",
        "            direct_correction_vector = act_corrected - act_wrong\n",
        "            uncertainty_vector = act_self_check - act_wrong\n",
        "\n",
        "            # Store steering vectors\n",
        "            steering_vectors[layer].append((direct_correction_vector, uncertainty_vector))\n",
        "\n",
        "            # Compute magnitude of change (Euclidean norm)\n",
        "            norm_change = np.linalg.norm(direct_correction_vector) + np.linalg.norm(uncertainty_vector)\n",
        "            layer_norm_changes[layer] += norm_change\n",
        "\n",
        "    # Compute final mean steering vectors per layer\n",
        "    final_steering_vectors = {}\n",
        "    for layer in range(num_layers):\n",
        "        direct_vectors = np.array([vec[0] for vec in steering_vectors[layer]])\n",
        "        uncertainty_vectors = np.array([vec[1] for vec in steering_vectors[layer]])\n",
        "\n",
        "        final_steering_vectors[layer] = {\n",
        "            \"direct\": np.mean(direct_vectors, axis=0),\n",
        "            \"uncertainty\": np.mean(uncertainty_vectors, axis=0),\n",
        "        }\n",
        "\n",
        "    # Find layer with maximum change\n",
        "    max_layer = np.argmax(layer_norm_changes)\n",
        "\n",
        "    return final_steering_vectors, max_layer, activations_per_layer\n"
      ],
      "metadata": {
        "id": "gXgQdh6Y2Vq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "csv_path_mdset = \"/content/drive/MyDrive/Articles /Math_Dset.xlsx\"\n",
        "dataset_df = pd.read_excel(csv_path_mdset, engine=\"openpyxl\")\n",
        "dataset = dataset_df.values.tolist()  # Convert to list of (Question, Positive Response, Negative Response)"
      ],
      "metadata": {
        "id": "1purr7WL1y0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "final_steering_vectors, max_layer, activations_per_layer = compute_steering_vector(dataset, model, tokenizer)\n"
      ],
      "metadata": {
        "id": "SKxIpjmy5Mck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from mpl_toolkits.mplot3d import Axes3D  # Import 3D plotting toolkit\n",
        "\n",
        "def plot_pca_variance_per_layer_3d(activations_per_layer, n_components=3):\n",
        "    \"\"\"\n",
        "    Performs PCA on activation differences across layers and plots a 3D scatter plot\n",
        "    distinguishing wrong, self-check, and corrected responses for each layer.\n",
        "\n",
        "    Parameters:\n",
        "    - activations_per_layer: Dictionary {layer: {wrong: Tensor, self_check: Tensor, corrected: Tensor}}\n",
        "    - n_components: Number of PCA components (default=3)\n",
        "\n",
        "    Returns:\n",
        "    - None (displays separate 3D scatter plots per layer)\n",
        "    \"\"\"\n",
        "    num_layers = len(activations_per_layer)\n",
        "\n",
        "    for layer in range(num_layers):\n",
        "        fig = plt.figure(figsize=(10, 8))\n",
        "        ax = fig.add_subplot(111, projection='3d')  # 3D subplot\n",
        "\n",
        "        # Extract activations for each stage\n",
        "        layer_acts = activations_per_layer[layer]\n",
        "        wrong_acts = np.array(layer_acts[\"wrong\"])\n",
        "        self_check_acts = np.array(layer_acts[\"self_check\"])\n",
        "        corrected_acts = np.array(layer_acts[\"corrected\"])\n",
        "\n",
        "        # Flatten if needed\n",
        "        wrong_acts = wrong_acts.reshape(wrong_acts.shape[0], -1)\n",
        "        self_check_acts = self_check_acts.reshape(self_check_acts.shape[0], -1)\n",
        "        corrected_acts = corrected_acts.reshape(corrected_acts.shape[0], -1)\n",
        "\n",
        "        print(f\"Layer {layer}: Wrong Variance - {np.var(wrong_acts, axis=0).sum()}\")\n",
        "\n",
        "        # Perform PCA with normalization\n",
        "        pca = PCA(n_components=n_components)\n",
        "        scaler = StandardScaler()\n",
        "        all_acts = scaler.fit_transform(np.vstack([wrong_acts, self_check_acts, corrected_acts]))  # Normalize activations\n",
        "        reduced_acts = pca.fit_transform(all_acts)\n",
        "        print(f\"Layer {layer}: PCA Explained Variance Ratio - {pca.explained_variance_ratio_}\")\n",
        "\n",
        "        # Split transformed activations back\n",
        "        split_1 = wrong_acts.shape[0]\n",
        "        split_2 = split_1 + self_check_acts.shape[0]\n",
        "        wrong_points = reduced_acts[:split_1]\n",
        "        self_check_points = reduced_acts[split_1:split_2]\n",
        "        corrected_points = reduced_acts[split_2:]\n",
        "\n",
        "        # 3D Scatter plot\n",
        "        ax.scatter(wrong_points[:, 0], wrong_points[:, 1], wrong_points[:, 2], color='blue', label=\"Wrong\", alpha=0.7)\n",
        "        ax.scatter(self_check_points[:, 0], self_check_points[:, 1], self_check_points[:, 2], color='orange', label=\"Self-Check\", alpha=0.7)\n",
        "        ax.scatter(corrected_points[:, 0], corrected_points[:, 1], corrected_points[:, 2], color='red', label=\"Corrected\", alpha=0.7)\n",
        "\n",
        "        # Labels & formatting\n",
        "        ax.set_xlabel(\"PCA Component 1\")\n",
        "        ax.set_ylabel(\"PCA Component 2\")\n",
        "        ax.set_zlabel(\"PCA Component 3\")\n",
        "        ax.set_title(f\"PCA 3D Scatter Plot - Layer {layer}\")\n",
        "        ax.legend()\n",
        "        ax.grid(True)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "# Example usage:\n",
        "# plot_pca_variance_per_layer_3d(activations_per_layer, n_components=3)\n"
      ],
      "metadata": {
        "id": "7JCt3T3m76mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('/content/drive/MyDrive/Articles /final_steering_vectors.npy', final_steering_vectors)"
      ],
      "metadata": {
        "id": "By4yQDdfKXs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying steering"
      ],
      "metadata": {
        "id": "QxhMYo9Q6XZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_cot_with_steering(model, tokenizer, question, steering_vectors, layer_to_modify, alpha):\n",
        "    \"\"\"\n",
        "    Generates CoT reasoning and applies activation steering **only when '=' is detected**.\n",
        "\n",
        "    Returns:\n",
        "    - str: Generated output with activation steering applied.\n",
        "    \"\"\"\n",
        "    cot_prompt = (\n",
        "        f\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\",\n",
        "        f\"The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.\",\n",
        "        f\"The reasoning process and answer are enclosed within <think> </think> and \",\n",
        "        f\"<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> \",\n",
        "        f\"<answer> answer here </answer>.\",\n",
        "        f\" The final answer must be a numeric or a decimal. \",\n",
        "        f\"User: {question}. Assistant: \"\n",
        "    )\n",
        "\n",
        "    cot_prompt = \" \".join(cot_prompt)\n",
        "\n",
        "    input_ids = tokenizer.encode(cot_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    generated_ids = []\n",
        "    steering_triggered = False\n",
        "\n",
        "    def hook_fn(module, input, output):\n",
        "        if steering_triggered:\n",
        "            return output + alpha * steering_vectors[layer_to_modify]  # Apply steering vector\n",
        "        return output  # No modification before '=' appears\n",
        "\n",
        "    handle = model.model.layers[layer_to_modify].mlp.register_forward_hook(hook_fn)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        while True:\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "            next_token_id = torch.argmax(logits, dim=-1).item()\n",
        "            generated_ids.append(next_token_id)\n",
        "\n",
        "            if tokenizer.decode([next_token_id]) == \"=\":\n",
        "                steering_triggered = True  # Activate steering after \"=\"\n",
        "\n",
        "            if next_token_id == tokenizer.eos_token_id or len(generated_ids) > 512:\n",
        "                break\n",
        "\n",
        "            input_ids = torch.cat([input_ids, torch.tensor([[next_token_id]]).to(model.device)], dim=-1)\n",
        "\n",
        "    handle.remove()\n",
        "    return tokenizer.decode(generated_ids)\n",
        "\n"
      ],
      "metadata": {
        "id": "EF8-l794DTjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_cot_answer(model, tokenizer, question, max_tokens=512):\n",
        "    \"\"\"\n",
        "    Generates a step-by-step Chain-of-Thought (CoT) reasoning response for a given question.\n",
        "\n",
        "    Args:\n",
        "    - model: Language model.\n",
        "    - tokenizer: Tokenizer for the model.\n",
        "    - question (str): The input question.\n",
        "    - max_tokens (int): Maximum number of tokens to generate.\n",
        "\n",
        "    Returns:\n",
        "    - str: The generated CoT response.\n",
        "    \"\"\"\n",
        "    cot_prompt = (\n",
        "        f\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\",\n",
        "        f\"The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.\",\n",
        "        f\"The reasoning process and answer are enclosed within <think> </think> and \",\n",
        "        f\"<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> \",\n",
        "        f\"<answer> answer here </answer>.\",\n",
        "        f\" The final answer must be a numeric or a decimal. \",\n",
        "        f\"User: {question}. Assistant: \"\n",
        "    )\n",
        "\n",
        "    cot_prompt = \" \".join(cot_prompt)\n",
        "\n",
        "    input_ids = tokenizer.encode(cot_prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(input_ids, max_length=max_tokens, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "c-qco_JJJZfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_cot_with_activation_steering(\n",
        "    dset: list,\n",
        "    csv_path: str,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    steering_vectors,\n",
        "    layer_to_modify: int,\n",
        "    alpha: float,\n",
        "    num_runs: int,\n",
        "    total: int\n",
        "):\n",
        "    \"\"\"\n",
        "    Runs step-by-step reasoning with and without activation steering.\n",
        "\n",
        "    Args:\n",
        "    - dset (list): Dataset containing questions and answers.\n",
        "    - csv_path (str): Path to save the results as a CSV.\n",
        "    - model: Language model.\n",
        "    - tokenizer: Tokenizer for the model.\n",
        "    - steering_vectors (dict): Precomputed steering vectors per layer.\n",
        "    - layer_to_modify (int): Layer where activation steering is applied.\n",
        "    - alpha (float): Strength of steering intervention.\n",
        "    - num_runs (int): Number of runs to repeat.\n",
        "    - total (int): Number of problems to evaluate.\n",
        "\n",
        "    Returns:\n",
        "    - df (DataFrame): Final results dataframe.\n",
        "    - accuracies (list): Accuracy per run.\n",
        "    \"\"\"\n",
        "    correct_counts_no_steering = [0] * num_runs\n",
        "    correct_counts_steering = [0] * num_runs\n",
        "    results = []\n",
        "\n",
        "    write_headers = not os.path.exists(csv_path)\n",
        "\n",
        "    for k in range(num_runs):\n",
        "        run_results = []\n",
        "\n",
        "        for i in tqdm(range(total), desc=f\"Run {k+1}/{num_runs}\"):\n",
        "            question = dset[i][\"question\"]\n",
        "            true_answer = extract_number(dset[i][\"answer\"])\n",
        "\n",
        "            # Generate normal CoT (without activation steering)\n",
        "            original_output = generate_cot_answer(model, tokenizer, question)\n",
        "\n",
        "            # Apply activation steering **only when '=' appears**\n",
        "            steered_output = generate_cot_with_steering(\n",
        "                model, tokenizer, question, steering_vectors, layer_to_modify, alpha\n",
        "            )\n",
        "\n",
        "            # Extract numerical answers\n",
        "            original_answer = extract_number(original_output)\n",
        "            steered_answer = extract_number(steered_output)\n",
        "\n",
        "            # Check correctness\n",
        "            is_correct_no_steering = (true_answer == original_answer)\n",
        "            is_correct_steering = (true_answer == steered_answer)\n",
        "\n",
        "            if is_correct_no_steering:\n",
        "                correct_counts_no_steering[k] += 1\n",
        "            if is_correct_steering:\n",
        "                correct_counts_steering[k] += 1\n",
        "\n",
        "            # Store results\n",
        "            run_results.append({\n",
        "                \"run_id\": k + 1,\n",
        "                \"sample_id\": i + 1,\n",
        "                \"question\": question,\n",
        "                \"answer_response\": dset[i][\"answer\"],\n",
        "                \"ground_truth\": true_answer,\n",
        "                \"cot_response_no_steering\": original_output,\n",
        "                \"cot_response_steering\": steered_output,\n",
        "                \"is_correct_no_steering\": is_correct_no_steering,\n",
        "                \"is_correct_steering\": is_correct_steering\n",
        "            })\n",
        "\n",
        "            # Save incrementally after every iteration\n",
        "            df = pd.DataFrame(run_results)\n",
        "            df.to_csv(csv_path, mode=\"a\", header=write_headers, index=False)\n",
        "            write_headers = False  # Ensure headers are written only once\n",
        "\n",
        "        results.append(run_results)\n",
        "\n",
        "    # Compute and print accuracy per run\n",
        "    accuracies_no_steering = [correct / total for correct in correct_counts_no_steering]\n",
        "    accuracies_steering = [correct / total for correct in correct_counts_steering]\n",
        "\n",
        "    for k in range(num_runs):\n",
        "        print(f\"Run {k+1} Accuracy (No Steering): {accuracies_no_steering[k]:.2%}\")\n",
        "        print(f\"Run {k+1} Accuracy (With Steering): {accuracies_steering[k]:.2%}\")\n",
        "\n",
        "    return pd.DataFrame(results), accuracies_no_steering, accuracies_steering\n"
      ],
      "metadata": {
        "id": "hZi1nIn5HmR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# File paths in Google Drive\n",
        "csv_path_3 = \"/content/drive/MyDrive/Articles /with_steer.csv\"\n",
        "\n",
        "# Check if JSON file exists\n",
        "if os.path.exists(csv_path_3):\n",
        "    print(\"File already exists. Loading previous results...\")\n",
        "    df_gsm8k = pd.read_csv(csv_path_3)\n",
        "\n",
        "    print(df_gsm8k.head())  # Show first few rows\n",
        "\n",
        "else:\n",
        "    print(\"No existing file found. Running CoT reasoning...\")\n",
        "    df_gsm8k = run_cot_with_activation_steering(gsm8k_ds_test, csv_path_3, model, tokenizer, final_steering_vectors, layer_to_modify=27, alpha=10, num_runs=1, total=5)\n",
        "\n"
      ],
      "metadata": {
        "id": "2YTKNDKmIISc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample example- With partial steering- Token triggered\n",
        "\n",
        "Attempted to execute this but ran into difficulties due to large inconcsistencies of output between model.generate() and model(). Additionally ths code is very slow for inferencing without the efficiencies of model.generate()"
      ],
      "metadata": {
        "id": "GZ7a9ZNo7Dz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Load the steering vector\n",
        "steering_vector = np.load(\"/content/drive/MyDrive/AI Assignment/MATS- Neel Nanda/Spring 2025/final_steering_vectors.npy\", allow_pickle=True)\n",
        "steering_vector = steering_vector.item()\n",
        "\n",
        "print(type(steering_vector))  # Check if it's a dict or list\n",
        "print(steering_vector)        # Print a portion of the data\n"
      ],
      "metadata": {
        "id": "9EHvfqR2wrvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_key = 28  # Change this to select a different vector\n",
        "steering_vector = steering_vector[selected_key][\"uncertainty\"]\n",
        "\n",
        "# Ensure it's a float32 array\n",
        "steering_vector = torch.tensor(steering_vector, dtype=torch.float32, device=device).to(device)"
      ],
      "metadata": {
        "id": "3_nfDevxw8q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm  # Import tqdm for progress tracking\n",
        "\n",
        "def generate_reasoning_output(prompt, alpha, apply_steering=False,max_tokens=200,num_steer=5):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    generated_tokens = []  # Store generated token IDs\n",
        "\n",
        "    if apply_steering:\n",
        "            # Ensure steering vector is on CUDA and properly reshaped\n",
        "            steering_vector_torch = torch.tensor(\n",
        "                steering_vector, dtype=torch.float32, device=device\n",
        "            ).view(1, 1, -1)\n",
        "\n",
        "    current_input = inputs[\"input_ids\"].to(device)  # Ensure input is on CUDA\n",
        "\n",
        "    decoded = tokenizer.decode(current_input[0].tolist(), skip_special_tokens=True)\n",
        "    print(decoded)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      output = model.generate(\n",
        "            input_ids=current_input,\n",
        "            max_new_tokens=1,  # Generate only one token per step\n",
        "            return_dict_in_generate=True,  # Get additional info like scores\n",
        "            output_hidden_states=True  # Get hidden states if needed\n",
        "            )\n",
        "\n",
        "      current_input = output.sequences\n",
        "      next_token = output.sequences[:, -1:]\n",
        "      generated_tokens.append(next_token.item())\n",
        "\n",
        "      print(type(output.sequences))\n",
        "      print(output.sequences.shape)\n",
        "\n",
        "      for _ in tqdm(range(max_tokens), desc=\"Generating Tokens\", unit=\"token\"):\n",
        "            output = model.generate(\n",
        "            input_ids=current_input,\n",
        "            max_new_tokens=1,  # Generate only one token per step\n",
        "            return_dict_in_generate=True,  # Get additional info like scores\n",
        "            output_hidden_states=True  # Get hidden states if needed\n",
        "            )\n",
        "\n",
        "            # Extract the newly generated token\n",
        "            next_token = output.sequences[:, -1:]  # Get only the new token\n",
        "\n",
        "            # Append token to sequence\n",
        "            generated_tokens.append(next_token.item())\n",
        "\n",
        "            # Update current_input\n",
        "            current_input = torch.cat([current_input, next_token], dim=1)\n",
        "\n",
        "            # decoded_text = tokenizer.decode(current_input[0].tolist(), skip_special_tokens=True)\n",
        "            # print(decoded_text)\n",
        "\n",
        "            # Decode only the last 5 tokens\n",
        "            decoded_last_five_tokens = tokenizer.decode(generated_tokens[-(num_steer):], skip_special_tokens=True)\n",
        "\n",
        "            # Check for punctuation only in the last 5 tokens\n",
        "            punctuation_flag = any(p in decoded_last_five_tokens for p in \".!?\")\n",
        "\n",
        "            if apply_steering and punctuation_flag:  # Apply steering at punctuation\n",
        "                print(\"Applied steer\")\n",
        "                # Get last layer's hidden states (shape: batch_size x seq_len x hidden_dim)\n",
        "                print(\"Otput.hidden_states: \",len(output.hidden_states[-1]))\n",
        "                hidden_states = output.hidden_states[-1]\n",
        "\n",
        "                # Get the last layer's hidden states\n",
        "                last_layer_hidden_states = hidden_states[-1]  # Last layer in the tuple\n",
        "\n",
        "                # Extract the last token's hidden state (index -1 for last token)\n",
        "                last_token_hidden_state = last_layer_hidden_states[:, -1, :].to(device)\n",
        "                print(\"Last_token_hidden_state_size: \", last_token_hidden_state.shape)\n",
        "\n",
        "                # Apply steering\n",
        "                steered_hidden_states = last_token_hidden_state + alpha * steering_vector_torch\n",
        "\n",
        "                full_embeds = model.get_input_embeddings()(current_input).clone()\n",
        "                full_embeds[:, -1, :] = steered_hidden_states.clone()\n",
        "\n",
        "                new_outputs = model(\n",
        "                                inputs_embeds=full_embeds,\n",
        "                                attention_mask = torch.ones_like(current_input, device=device)  # Ensure padding tokens are ignored\n",
        "                                    )\n",
        "\n",
        "                logits = new_outputs.logits[:, -1, :]  # Get updated logits\n",
        "\n",
        "                # **Choose new token from steered logits**\n",
        "                next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "                # Append steered token to sequence\n",
        "                generated_tokens[-1] = next_token.item()\n",
        "\n",
        "                current_input[:, -1] = next_token  # Replace last token with next_token\n",
        "\n",
        "              # Append instead of overwrite\n",
        "            # print(current_input.shape)\n",
        "\n",
        "            # Stop if EOS token is reached\n",
        "            if next_token.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    return tokenizer.decode(generated_tokens, skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "tvP_gPlX7Giu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"do_sample: {getattr(model.config, 'do_sample', 'Not set')}\")\n",
        "print(f\"temperature: {getattr(model.config, 'temperature', 'Not set')}\")\n",
        "print(f\"top_k: {getattr(model.config, 'top_k', 'Not set')}\")\n",
        "print(f\"top_p: {getattr(model.config, 'top_p', 'Not set')}\")\n"
      ],
      "metadata": {
        "id": "48lMI78MPh5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example multi-step reasoning prompt\n",
        "prompt = \"Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make? Solve step by step, giving final answer as a numeric\"\n",
        "baseline_output = generate_reasoning_output(prompt, alpha =0, apply_steering=False,max_tokens=50)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "iwrQCz9c2d4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate outputs\n",
        "steered_output_1 = generate_reasoning_output(prompt, alpha = 0, apply_steering=True,max_tokens=50)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "k7VX4obzAyJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steered_output_2 = generate_reasoning_output(prompt, alpha = 0.01, apply_steering=True,max_tokens=350)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CIcH1KxbDQVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steered_output_3 = generate_reasoning_output(prompt, alpha = 0.0001, apply_steering=True,max_tokens=350,num_steer=2)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LOOaLivgFtEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steered_output_4 = generate_reasoning_output(prompt, alpha = 0.00000001, apply_steering=True,max_tokens=350,num_steer=2)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TDd_YuohGWvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steered_output_5 = generate_reasoning_output(prompt, alpha = 0, apply_steering=True,max_tokens=350)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "367hSe57HF2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steered_output_6 = generate_reasoning_output(prompt, alpha = 0.02, apply_steering=True,max_tokens=350,num_steer=3)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JgXdo5kXHM1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steered_output_7 = generate_reasoning_output(prompt, alpha = 0, apply_steering=True,max_tokens=350,num_steer=2)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "duOFHbuFIi4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steered_output_8 = generate_reasoning_output(prompt, alpha = 0.025, apply_steering=True,max_tokens=350,num_steer=6)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "a5UBPBVoV--a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steered_output_9 = generate_reasoning_output(prompt, alpha = 0.025, apply_steering=True,max_tokens=350,num_steer=3)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EUxzTy5NXCx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display results\n",
        "print(\"=== Baseline Output ===\\n\", baseline_output)\n",
        "print(\"\\n=== Steered Output 1, alpha =0 ===\\n\", steered_output_1)\n",
        "#print(\"\\n=== Steered Output 2, alpha =0.01 ===\\n\", steered_output_2)\n",
        "#print(\"\\n=== Steered Output 3, alpha =0.0001,num_steer=2 ===\\n\", steered_output_3)\n",
        "#print(\"\\n=== Steered Output 4, alpha =0.00000001,num_steer=2 ===\\n\", steered_output_4)\n",
        "#print(\"\\n=== Steered Output 5, alpha =0 ===\\n\", steered_output_5)\n",
        "#print(\"\\n=== Steered Output 6, alpha =0.02,num_steer=3 ===\\n\", steered_output_6)\n",
        "#print(\"\\n=== Steered Output 7, alpha =0,num_steer=2 ===\\n\", steered_output_7)\n",
        "#print(\"\\n=== Steered Output 8, alpha =0.025,num_steer=6 ===\\n\", steered_output_8)\n",
        "#print(\"\\n=== Steered Output 9, alpha =0.025,num_steer=3 ===\\n\", steered_output_9)"
      ],
      "metadata": {
        "id": "ytUAFJEh_6MV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steered_output_2 = generate_reasoning_output(prompt, alpha = 1, apply_steering=True)\n",
        "steered_output_3 = generate_reasoning_output(prompt, alpha = 0.00000001, apply_steering=True)\n",
        "\n",
        "print(\"\\n=== Steered Output 2 ===\\n\", steered_output_2)\n",
        "print(\"\\n=== Steered Output 3 ===\\n\", steered_output_3)"
      ],
      "metadata": {
        "id": "Q_hQryGExueN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample example- Full steering\n",
        "\n",
        "Here steering is applied throught the generation process resulting in more stable outputs"
      ],
      "metadata": {
        "id": "BqwfomhpFe10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "steering_vector = np.load(\"/content/drive/MyDrive/Articles /final_steering_vectors.npy\", allow_pickle=True)\n",
        "steering_vector = steering_vector.item()\n",
        "\n",
        "selected_key = 28  # Change this to select a different vector\n",
        "steering_vector = steering_vector[selected_key][\"uncertainty\"]\n",
        "\n",
        "# Ensure it's a float32 array\n",
        "steering_vector = torch.tensor(steering_vector, dtype=torch.float32, device=device).to(device)"
      ],
      "metadata": {
        "id": "0XNEq7XvtXS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "def apply_steering_hook(model, tokenizer, lookback=5, alpha=0.5):\n",
        "    \"\"\"\n",
        "    Applies a steering hook to modify hidden states if punctuation appears within the last `lookback` tokens.\n",
        "\n",
        "    Parameters:\n",
        "        model (transformers.PreTrainedModel): The causal language model.\n",
        "        tokenizer (transformers.PreTrainedTokenizer): Tokenizer for the model.\n",
        "        lookback (int): Number of tokens to check for punctuation (default: 5).\n",
        "        alpha (float): Strength of steering modification (default: 0.5).\n",
        "\n",
        "    Returns:\n",
        "        hook_handle (torch.utils.hooks.RemovableHandle): Handle to remove the hook after generation.\n",
        "    \"\"\"\n",
        "\n",
        "    def forward_hook(module, input, output):\n",
        "        output = output.to(steering_vector.device)\n",
        "        output[:, -1, :] += alpha * steering_vector\n",
        "\n",
        "        return output\n",
        "\n",
        "    # Attach hook to the last layer's `post_attention_layernorm`\n",
        "    last_layer = model.model.layers[-1].post_attention_layernorm\n",
        "    return last_layer.register_forward_hook(forward_hook)\n",
        "\n",
        "\n",
        "def generate_with_steering(model, tokenizer, prompt, lookback=5, alpha=0.5, max_new_tokens=50):\n",
        "    \"\"\"\n",
        "    Generates text using `model.generate()` with a conditional steering hook.\n",
        "\n",
        "    Parameters:\n",
        "        model (transformers.PreTrainedModel): The causal language model.\n",
        "        tokenizer (transformers.PreTrainedTokenizer): Tokenizer for the model.\n",
        "        prompt (str): The input prompt.\n",
        "        lookback (int): Number of tokens to check for punctuation (default: 5).\n",
        "        alpha (float): Strength of steering modification (default: 0.5).\n",
        "        max_new_tokens (int): Number of tokens to generate (default: 50).\n",
        "\n",
        "    Returns:\n",
        "        str: The generated text.\n",
        "    \"\"\"\n",
        "    hook_handle = apply_steering_hook(model, tokenizer, lookback, alpha)\n",
        "    print(\"Hook handle: \",hook_handle)\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
        "\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        return_dict_in_generate=True\n",
        "    )\n",
        "\n",
        "    hook_handle.remove()  # Remove the hook to avoid affecting future generations\n",
        "\n",
        "    return tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "jEwJa1A4t6mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make? Solve step by step, giving final answer as a numeric\"\n",
        "gen_text_base = generate_with_steering(model, tokenizer, prompt, lookback=5, alpha=0, max_new_tokens=500)"
      ],
      "metadata": {
        "id": "exX-s_XguJtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_text_steer_1 = generate_with_steering(model, tokenizer, prompt, lookback=5, alpha=0.001, max_new_tokens=500)"
      ],
      "metadata": {
        "id": "5PtjMbWHB354"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_text_steer_2 = generate_with_steering(model, tokenizer, prompt, lookback=5, alpha=0.01, max_new_tokens=500)"
      ],
      "metadata": {
        "id": "M2UhjwjlCMN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_text_steer_3 = generate_with_steering(model, tokenizer, prompt, lookback=5, alpha=0.1, max_new_tokens=500)"
      ],
      "metadata": {
        "id": "SIV2gW1rCOiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_text_steer_4 = generate_with_steering(model, tokenizer, prompt, lookback=5, alpha=1, max_new_tokens=500)"
      ],
      "metadata": {
        "id": "B0l-UkMiCpqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_text_steer_5 = generate_with_steering(model, tokenizer, prompt, lookback=5, alpha=10, max_new_tokens=500)"
      ],
      "metadata": {
        "id": "7qiUW7SeDHFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"_____________Base text_____________________\\n\",gen_text_base)\n",
        "print(\"\\n_____________Steer text1_____________________\\n\",gen_text_steer_1)\n",
        "print(\"\\n_____________Steer text2_____________________\\n\",gen_text_steer_2)\n",
        "print(\"\\n_____________Steer text3_____________________\\n\",gen_text_steer_3)\n",
        "print(\"\\n_____________Steer text4_____________________\\n\",gen_text_steer_4)\n",
        "print(\"\\n_____________Steer text5_____________________\\n\",gen_text_steer_5)"
      ],
      "metadata": {
        "id": "b_JvvHIZ_vjd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "cKqBBVAKaJsO",
        "5FcVjlVgQkR2"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
